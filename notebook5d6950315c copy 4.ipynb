{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading: train_t15.2023.08.11\n",
      "Loading: test_t15.2023.08.13\n",
      "Loading: train_t15.2023.08.13\n",
      "Loading: val_t15.2023.08.13\n",
      "Loading: test_t15.2023.08.18\n",
      "Loading: train_t15.2023.08.18\n",
      "Loading: val_t15.2023.08.18\n",
      "Loading: test_t15.2023.08.20\n",
      "Loading: train_t15.2023.08.20\n",
      "Loading: val_t15.2023.08.20\n",
      "Loading: test_t15.2023.08.25\n",
      "Loading: train_t15.2023.08.25\n",
      "Loading: val_t15.2023.08.25\n",
      "Loading: test_t15.2023.08.27\n",
      "Loading: train_t15.2023.08.27\n",
      "Loading: val_t15.2023.08.27\n",
      "Loading: test_t15.2023.09.01\n",
      "Loading: train_t15.2023.09.01\n",
      "Loading: val_t15.2023.09.01\n",
      "Loading: test_t15.2023.09.03\n",
      "Loading: train_t15.2023.09.03\n",
      "Loading: val_t15.2023.09.03\n",
      "Loading: test_t15.2023.09.24\n",
      "Loading: train_t15.2023.09.24\n",
      "Loading: val_t15.2023.09.24\n",
      "Loading: test_t15.2023.09.29\n",
      "Loading: train_t15.2023.09.29\n",
      "Loading: val_t15.2023.09.29\n",
      "Loading: test_t15.2023.10.01\n",
      "Loading: train_t15.2023.10.01\n",
      "Loading: val_t15.2023.10.01\n",
      "Loading: test_t15.2023.10.06\n",
      "Loading: train_t15.2023.10.06\n",
      "Loading: val_t15.2023.10.06\n",
      "Loading: test_t15.2023.10.08\n",
      "Loading: train_t15.2023.10.08\n",
      "Loading: val_t15.2023.10.08\n",
      "Loading: test_t15.2023.10.13\n",
      "Loading: train_t15.2023.10.13\n",
      "Loading: val_t15.2023.10.13\n",
      "Loading: test_t15.2023.10.15\n",
      "Loading: train_t15.2023.10.15\n",
      "Loading: val_t15.2023.10.15\n",
      "Loading: test_t15.2023.10.20\n",
      "Loading: train_t15.2023.10.20\n",
      "Loading: val_t15.2023.10.20\n",
      "Loading: test_t15.2023.10.22\n",
      "Loading: train_t15.2023.10.22\n",
      "Loading: val_t15.2023.10.22\n",
      "Loading: test_t15.2023.11.03\n",
      "Loading: train_t15.2023.11.03\n",
      "Loading: val_t15.2023.11.03\n",
      "Loading: test_t15.2023.11.04\n",
      "Loading: train_t15.2023.11.04\n",
      "Loading: val_t15.2023.11.04\n",
      "Loading: test_t15.2023.11.17\n",
      "Loading: train_t15.2023.11.17\n",
      "Loading: val_t15.2023.11.17\n",
      "Loading: test_t15.2023.11.19\n",
      "Loading: train_t15.2023.11.19\n",
      "Loading: val_t15.2023.11.19\n",
      "Loading: test_t15.2023.11.26\n",
      "Loading: train_t15.2023.11.26\n",
      "Loading: val_t15.2023.11.26\n",
      "Loading: test_t15.2023.12.03\n",
      "Loading: train_t15.2023.12.03\n",
      "Loading: val_t15.2023.12.03\n",
      "Loading: test_t15.2023.12.08\n",
      "Loading: train_t15.2023.12.08\n",
      "Loading: val_t15.2023.12.08\n",
      "Loading: test_t15.2023.12.10\n",
      "Loading: train_t15.2023.12.10\n",
      "Loading: val_t15.2023.12.10\n",
      "Loading: test_t15.2023.12.17\n",
      "Loading: train_t15.2023.12.17\n",
      "Loading: val_t15.2023.12.17\n",
      "Loading: test_t15.2023.12.29\n",
      "Loading: train_t15.2023.12.29\n",
      "Loading: val_t15.2023.12.29\n",
      "Loading: test_t15.2024.02.25\n",
      "Loading: train_t15.2024.02.25\n",
      "Loading: val_t15.2024.02.25\n",
      "Loading: train_t15.2024.03.03\n",
      "Loading: test_t15.2024.03.08\n",
      "Loading: train_t15.2024.03.08\n",
      "Loading: val_t15.2024.03.08\n",
      "Loading: test_t15.2024.03.15\n",
      "Loading: train_t15.2024.03.15\n",
      "Loading: val_t15.2024.03.15\n",
      "Loading: test_t15.2024.03.17\n",
      "Loading: train_t15.2024.03.17\n",
      "Loading: val_t15.2024.03.17\n",
      "Loading: train_t15.2024.04.25\n",
      "Loading: train_t15.2024.04.28\n",
      "Loading: test_t15.2024.05.10\n",
      "Loading: train_t15.2024.05.10\n",
      "Loading: val_t15.2024.05.10\n",
      "Loading: test_t15.2024.06.14\n",
      "Loading: train_t15.2024.06.14\n",
      "Loading: val_t15.2024.06.14\n",
      "Loading: test_t15.2024.07.19\n",
      "Loading: train_t15.2024.07.19\n",
      "Loading: val_t15.2024.07.19\n",
      "Loading: test_t15.2024.07.21\n",
      "Loading: train_t15.2024.07.21\n",
      "Loading: val_t15.2024.07.21\n",
      "Loading: test_t15.2024.07.28\n",
      "Loading: train_t15.2024.07.28\n",
      "Loading: val_t15.2024.07.28\n",
      "Loading: test_t15.2025.01.10\n",
      "Loading: train_t15.2025.01.10\n",
      "Loading: val_t15.2025.01.10\n",
      "Loading: test_t15.2025.01.12\n",
      "Loading: train_t15.2025.01.12\n",
      "Loading: val_t15.2025.01.12\n",
      "Loading: test_t15.2025.03.14\n",
      "Loading: train_t15.2025.03.14\n",
      "Loading: val_t15.2025.03.14\n",
      "Loading: test_t15.2025.03.16\n",
      "Loading: train_t15.2025.03.16\n",
      "Loading: val_t15.2025.03.16\n",
      "Loading: test_t15.2025.03.30\n",
      "Loading: train_t15.2025.03.30\n",
      "Loading: val_t15.2025.03.30\n",
      "Loading: test_t15.2025.04.13\n",
      "Loading: train_t15.2025.04.13\n",
      "Loading: val_t15.2025.04.13\n",
      "Keys in data_dict: ['train_t15.2023.08.11', 'test_t15.2023.08.13', 'train_t15.2023.08.13', 'val_t15.2023.08.13', 'test_t15.2023.08.18', 'train_t15.2023.08.18', 'val_t15.2023.08.18', 'test_t15.2023.08.20', 'train_t15.2023.08.20', 'val_t15.2023.08.20']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import h5py\n",
    "import pandas as pd\n",
    "\n",
    "# ---------- 1. helper to load ONE .hdf5 file (same as Anna) ----------\n",
    "def load_h5py_file(file_path):\n",
    "    data = {\n",
    "        'neural_features': [],\n",
    "        'n_time_steps': [],\n",
    "        'seq_class_ids': [],\n",
    "        'seq_len': [],\n",
    "        'transcriptions': [],\n",
    "        'sentence_label': [],\n",
    "        'session': [],\n",
    "        'block_num': [],\n",
    "        'trial_num': [],\n",
    "    }\n",
    "    with h5py.File(file_path, 'r') as f:\n",
    "        for key in f.keys():\n",
    "            g = f[key]\n",
    "\n",
    "            neural_features = g['input_features'][:]\n",
    "            n_time_steps = g.attrs['n_time_steps']\n",
    "            seq_class_ids = g['seq_class_ids'][:] if 'seq_class_ids' in g else None\n",
    "            seq_len = g.attrs['seq_len'] if 'seq_len' in g.attrs else None\n",
    "            transcription = g['transcription'][:] if 'transcription' in g else None\n",
    "            sentence_label = g.attrs['sentence_label'][:] if 'sentence_label' in g.attrs else None\n",
    "            session = g.attrs['session']\n",
    "            block_num = g.attrs['block_num']\n",
    "            trial_num = g.attrs['trial_num']\n",
    "\n",
    "            data['neural_features'].append(neural_features)\n",
    "            data['n_time_steps'].append(n_time_steps)\n",
    "            data['seq_class_ids'].append(seq_class_ids)\n",
    "            data['seq_len'].append(seq_len)\n",
    "            data['transcriptions'].append(transcription)\n",
    "            data['sentence_label'].append(sentence_label)\n",
    "            data['session'].append(session)\n",
    "            data['block_num'].append(block_num)\n",
    "            data['trial_num'].append(trial_num)\n",
    "    return data\n",
    "\n",
    "# ---------- 2. walk local /input like Kaggle walks /kaggle/input ----------\n",
    "INPUT_ROOT = Path(\"..\") / \"input\"       # this folder contains \"brain-to-text-25\"\n",
    "\n",
    "data_dict = {}\n",
    "\n",
    "for dirname, _, filenames in os.walk(INPUT_ROOT):\n",
    "    for filename in filenames:\n",
    "        if filename.endswith(\".hdf5\"):\n",
    "            # exactly the same key logic as Anna’s file :contentReference[oaicite:0]{index=0}\n",
    "            dict_key_util = \"_\".join([\n",
    "                filename.split('.')[0].split('_')[-1],   # train / val / test\n",
    "                dirname.split(os.sep)[-1]               # t15.2023.09.29\n",
    "            ])\n",
    "\n",
    "            file_path = os.path.join(dirname, filename)\n",
    "            print(\"Loading:\", dict_key_util)\n",
    "            data_dict[dict_key_util] = load_h5py_file(file_path)\n",
    "\n",
    "print(\"Keys in data_dict:\", list(data_dict.keys())[:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-11-06T10:06:20.063188Z",
     "iopub.status.busy": "2025-11-06T10:06:20.062811Z",
     "iopub.status.idle": "2025-11-06T10:06:20.070149Z",
     "shell.execute_reply": "2025-11-06T10:06:20.068760Z",
     "shell.execute_reply.started": "2025-11-06T10:06:20.063156Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import os\n",
    "\n",
    "LOGIT_TO_PHONEME = [\n",
    "'BLANK',    # \"BLANK\" = CTC blank symbol\n",
    "'AA', 'AE', 'AH', 'AO', 'AW',\n",
    "'AY', 'B', 'CH', 'D', 'DH',\n",
    "'EH', 'ER', 'EY', 'F', 'G',\n",
    "'HH', 'IH', 'IY', 'JH', 'K',\n",
    "'L', 'M', 'N', 'NG', 'OW',\n",
    "'OY', 'P', 'R', 'S', 'SH',\n",
    "'T', 'TH', 'UH', 'UW', 'V',\n",
    "'W', 'Y', 'Z', 'ZH',\n",
    "' | ',    # \"|\" = silence token\n",
    "]\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-06T10:06:26.262969Z",
     "iopub.status.busy": "2025-11-06T10:06:26.262570Z",
     "iopub.status.idle": "2025-11-06T10:06:26.321842Z",
     "shell.execute_reply": "2025-11-06T10:06:26.320471Z",
     "shell.execute_reply.started": "2025-11-06T10:06:26.262941Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import h5py\n",
    "\n",
    "def load_h5py_file(file_path):\n",
    "    data = {\n",
    "        'neural_features': [],\n",
    "        'n_time_steps': [],\n",
    "        'seq_class_ids': [],\n",
    "        'seq_len': [],\n",
    "        'transcriptions': [],\n",
    "        'sentence_label': [],\n",
    "        'session': [],\n",
    "        'block_num': [],\n",
    "        'trial_num': [],\n",
    "    }\n",
    "    # Open the hdf5 file for that day\n",
    "    with h5py.File(file_path, 'r') as f:\n",
    "\n",
    "        keys = list(f.keys())\n",
    "\n",
    "        # For each trial in the selected trials on that day\n",
    "        for key in keys:\n",
    "            g = f[key]\n",
    "\n",
    "            neural_features = g['input_features'][:]\n",
    "            n_time_steps = g.attrs['n_time_steps']\n",
    "            seq_class_ids = g['seq_class_ids'][:] if 'seq_class_ids' in g else None\n",
    "            seq_len = g.attrs['seq_len'] if 'seq_len' in g.attrs else None\n",
    "            transcription = g['transcription'][:] if 'transcription' in g else None\n",
    "            sentence_label = g.attrs['sentence_label'][:] if 'sentence_label' in g.attrs else None\n",
    "            session = g.attrs['session']\n",
    "            block_num = g.attrs['block_num']\n",
    "            trial_num = g.attrs['trial_num']\n",
    "\n",
    "            data['neural_features'].append(neural_features)\n",
    "            data['n_time_steps'].append(n_time_steps)\n",
    "            data['seq_class_ids'].append(seq_class_ids)\n",
    "            data['seq_len'].append(seq_len)\n",
    "            data['transcriptions'].append(transcription)\n",
    "            data['sentence_label'].append(sentence_label)\n",
    "            data['session'].append(session)\n",
    "            data['block_num'].append(block_num)\n",
    "            data['trial_num'].append(trial_num)\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-06T10:06:29.710159Z",
     "iopub.status.busy": "2025-11-06T10:06:29.709477Z",
     "iopub.status.idle": "2025-11-06T10:11:09.583604Z",
     "shell.execute_reply": "2025-11-06T10:11:09.581670Z",
     "shell.execute_reply.started": "2025-11-06T10:06:29.710125Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "data_dict = {}\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        if ('.hdf5' in filename):\n",
    "            dict_key_util = '_'.join([filename.split('.')[0].split('_')[-1], dirname.split('/')[-1]])\n",
    "            data_dict[dict_key_util] = load_h5py_file(os.path.join(dirname, filename))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* neural_features: Temporally binned (20 ms) neural features for each trial (512 X T).\n",
    "* n_time_steps: Number of time steps per trial.\n",
    "* seq_class_ids: Integer phoneme sequence labels for each trial. Integers correspond to phonemes using the mapping according to **LOGIT_TO_PHONEME**\n",
    "* seq_len: Number of phoneme labels per trial.\n",
    "* transcriptions: ASCII representation of sentence label for each trial.\n",
    "* sentence_label: Raw text sentence label for each trial.\n",
    "* session: Date that the trial's data was collected. Each date has a number of blocks, each block has a number of trials.\n",
    "* block_num: Research block number that the trial is sourced from.\n",
    "* trial_num: Trial number that the trial is sourced from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATA_ROOT: C:\\Users\\btlim\\OneDrive\\Desktop\\brain_to_text_local\\input\\brain-to-text-25\\t15_copyTask_neuralData\\hdf5_data_final\n",
      "Loading: train_t15.2023.08.11\n",
      "Loading: test_t15.2023.08.13\n",
      "Loading: train_t15.2023.08.13\n",
      "Loading: val_t15.2023.08.13\n",
      "Loading: test_t15.2023.08.18\n",
      "Loading: train_t15.2023.08.18\n",
      "Loading: val_t15.2023.08.18\n",
      "Loading: test_t15.2023.08.20\n",
      "Loading: train_t15.2023.08.20\n",
      "Loading: val_t15.2023.08.20\n",
      "Loading: test_t15.2023.08.25\n",
      "Loading: train_t15.2023.08.25\n",
      "Loading: val_t15.2023.08.25\n",
      "Loading: test_t15.2023.08.27\n",
      "Loading: train_t15.2023.08.27\n",
      "Loading: val_t15.2023.08.27\n",
      "Loading: test_t15.2023.09.01\n",
      "Loading: train_t15.2023.09.01\n",
      "Loading: val_t15.2023.09.01\n",
      "Loading: test_t15.2023.09.03\n",
      "Loading: train_t15.2023.09.03\n",
      "Loading: val_t15.2023.09.03\n",
      "Loading: test_t15.2023.09.24\n",
      "Loading: train_t15.2023.09.24\n",
      "Loading: val_t15.2023.09.24\n",
      "Loading: test_t15.2023.09.29\n",
      "Loading: train_t15.2023.09.29\n",
      "Loading: val_t15.2023.09.29\n",
      "Loading: test_t15.2023.10.01\n",
      "Loading: train_t15.2023.10.01\n",
      "Loading: val_t15.2023.10.01\n",
      "Loading: test_t15.2023.10.06\n",
      "Loading: train_t15.2023.10.06\n",
      "Loading: val_t15.2023.10.06\n",
      "Loading: test_t15.2023.10.08\n",
      "Loading: train_t15.2023.10.08\n",
      "Loading: val_t15.2023.10.08\n",
      "Loading: test_t15.2023.10.13\n",
      "Loading: train_t15.2023.10.13\n",
      "Loading: val_t15.2023.10.13\n",
      "Loading: test_t15.2023.10.15\n",
      "Loading: train_t15.2023.10.15\n",
      "Loading: val_t15.2023.10.15\n",
      "Loading: test_t15.2023.10.20\n",
      "Loading: train_t15.2023.10.20\n",
      "Loading: val_t15.2023.10.20\n",
      "Loading: test_t15.2023.10.22\n",
      "Loading: train_t15.2023.10.22\n",
      "Loading: val_t15.2023.10.22\n",
      "Loading: test_t15.2023.11.03\n",
      "Loading: train_t15.2023.11.03\n",
      "Loading: val_t15.2023.11.03\n",
      "Loading: test_t15.2023.11.04\n",
      "Loading: train_t15.2023.11.04\n",
      "Loading: val_t15.2023.11.04\n",
      "Loading: test_t15.2023.11.17\n",
      "Loading: train_t15.2023.11.17\n",
      "Loading: val_t15.2023.11.17\n",
      "Loading: test_t15.2023.11.19\n",
      "Loading: train_t15.2023.11.19\n",
      "Loading: val_t15.2023.11.19\n",
      "Loading: test_t15.2023.11.26\n",
      "Loading: train_t15.2023.11.26\n",
      "Loading: val_t15.2023.11.26\n",
      "Loading: test_t15.2023.12.03\n",
      "Loading: train_t15.2023.12.03\n",
      "Loading: val_t15.2023.12.03\n",
      "Loading: test_t15.2023.12.08\n",
      "Loading: train_t15.2023.12.08\n",
      "Loading: val_t15.2023.12.08\n",
      "Loading: test_t15.2023.12.10\n",
      "Loading: train_t15.2023.12.10\n",
      "Loading: val_t15.2023.12.10\n",
      "Loading: test_t15.2023.12.17\n",
      "Loading: train_t15.2023.12.17\n",
      "Loading: val_t15.2023.12.17\n",
      "Loading: test_t15.2023.12.29\n",
      "Loading: train_t15.2023.12.29\n",
      "Loading: val_t15.2023.12.29\n",
      "Loading: test_t15.2024.02.25\n",
      "Loading: train_t15.2024.02.25\n",
      "Loading: val_t15.2024.02.25\n",
      "Loading: train_t15.2024.03.03\n",
      "Loading: test_t15.2024.03.08\n",
      "Loading: train_t15.2024.03.08\n",
      "Loading: val_t15.2024.03.08\n",
      "Loading: test_t15.2024.03.15\n",
      "Loading: train_t15.2024.03.15\n",
      "Loading: val_t15.2024.03.15\n",
      "Loading: test_t15.2024.03.17\n",
      "Loading: train_t15.2024.03.17\n",
      "Loading: val_t15.2024.03.17\n",
      "Loading: train_t15.2024.04.25\n",
      "Loading: train_t15.2024.04.28\n",
      "Loading: test_t15.2024.05.10\n",
      "Loading: train_t15.2024.05.10\n",
      "Loading: val_t15.2024.05.10\n",
      "Loading: test_t15.2024.06.14\n",
      "Loading: train_t15.2024.06.14\n",
      "Loading: val_t15.2024.06.14\n",
      "Loading: test_t15.2024.07.19\n",
      "Loading: train_t15.2024.07.19\n",
      "Loading: val_t15.2024.07.19\n",
      "Loading: test_t15.2024.07.21\n",
      "Loading: train_t15.2024.07.21\n",
      "Loading: val_t15.2024.07.21\n",
      "Loading: test_t15.2024.07.28\n",
      "Loading: train_t15.2024.07.28\n",
      "Loading: val_t15.2024.07.28\n",
      "Loading: test_t15.2025.01.10\n",
      "Loading: train_t15.2025.01.10\n",
      "Loading: val_t15.2025.01.10\n",
      "Loading: test_t15.2025.01.12\n",
      "Loading: train_t15.2025.01.12\n",
      "Loading: val_t15.2025.01.12\n",
      "Loading: test_t15.2025.03.14\n",
      "Loading: train_t15.2025.03.14\n",
      "Loading: val_t15.2025.03.14\n",
      "Loading: test_t15.2025.03.16\n",
      "Loading: train_t15.2025.03.16\n",
      "Loading: val_t15.2025.03.16\n",
      "Loading: test_t15.2025.03.30\n",
      "Loading: train_t15.2025.03.30\n",
      "Loading: val_t15.2025.03.30\n",
      "Loading: test_t15.2025.04.13\n",
      "Loading: train_t15.2025.04.13\n",
      "Loading: val_t15.2025.04.13\n",
      "Number of keys: 127\n",
      "First 10 keys: ['train_t15.2023.08.11', 'test_t15.2023.08.13', 'train_t15.2023.08.13', 'val_t15.2023.08.13', 'test_t15.2023.08.18', 'train_t15.2023.08.18', 'val_t15.2023.08.18', 'test_t15.2023.08.20', 'train_t15.2023.08.20', 'val_t15.2023.08.20']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import h5py\n",
    "import pandas as pd\n",
    "\n",
    "def load_h5py_file(file_path):\n",
    "    data = {\n",
    "        'neural_features': [],\n",
    "        'n_time_steps': [],\n",
    "        'seq_class_ids': [],\n",
    "        'seq_len': [],\n",
    "        'transcriptions': [],\n",
    "        'sentence_label': [],\n",
    "        'session': [],\n",
    "        'block_num': [],\n",
    "        'trial_num': [],\n",
    "    }\n",
    "    with h5py.File(file_path, 'r') as f:\n",
    "        for key in f.keys():\n",
    "            g = f[key]\n",
    "\n",
    "            neural_features = g['input_features'][:]\n",
    "            n_time_steps = g.attrs['n_time_steps']\n",
    "            seq_class_ids = g['seq_class_ids'][:] if 'seq_class_ids' in g else None\n",
    "            seq_len = g.attrs['seq_len'] if 'seq_len' in g.attrs else None\n",
    "            transcription = g['transcription'][:] if 'transcription' in g else None\n",
    "            sentence_label = g.attrs['sentence_label'][:] if 'sentence_label' in g.attrs else None\n",
    "            session = g.attrs['session']\n",
    "            block_num = g.attrs['block_num']\n",
    "            trial_num = g.attrs['trial_num']\n",
    "\n",
    "            data['neural_features'].append(neural_features)\n",
    "            data['n_time_steps'].append(n_time_steps)\n",
    "            data['seq_class_ids'].append(seq_class_ids)\n",
    "            data['seq_len'].append(seq_len)\n",
    "            data['transcriptions'].append(transcription)\n",
    "            data['sentence_label'].append(sentence_label)\n",
    "            data['session'].append(session)\n",
    "            data['block_num'].append(block_num)\n",
    "            data['trial_num'].append(trial_num)\n",
    "    return data\n",
    "\n",
    "# ✅ root of your local data (equivalent to /kaggle/input/brain-to-text-25/t15_copyTask_neuralData/hdf5_data_final)\n",
    "DATA_ROOT = Path(\"..\") / \"input\" / \"brain-to-text-25\" / \"t15_copyTask_neuralData\" / \"hdf5_data_final\"\n",
    "\n",
    "print(\"DATA_ROOT:\", DATA_ROOT.resolve())\n",
    "\n",
    "data_dict = {}\n",
    "\n",
    "for dirname, _, filenames in os.walk(DATA_ROOT):\n",
    "    for filename in filenames:\n",
    "        if filename.endswith(\".hdf5\"):\n",
    "            # 'train' / 'val' / 'test'\n",
    "            split_name = filename.split('.')[0].split('_')[-1]\n",
    "            # folder name like 't15.2023.09.29'\n",
    "            date_name = os.path.basename(dirname)\n",
    "            dict_key_util = \"_\".join([split_name, date_name])\n",
    "\n",
    "            file_path = os.path.join(dirname, filename)\n",
    "            print(\"Loading:\", dict_key_util)\n",
    "            data_dict[dict_key_util] = load_h5py_file(file_path)\n",
    "\n",
    "print(\"Number of keys:\", len(data_dict))\n",
    "print(\"First 10 keys:\", list(data_dict.keys())[:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-06T10:01:16.393503Z",
     "iopub.status.busy": "2025-11-06T10:01:16.393160Z",
     "iopub.status.idle": "2025-11-06T10:01:16.705296Z",
     "shell.execute_reply": "2025-11-06T10:01:16.704110Z",
     "shell.execute_reply.started": "2025-11-06T10:01:16.393478Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['train_t15.2023.08.11', 'test_t15.2023.08.13', 'train_t15.2023.08.13', 'val_t15.2023.08.13', 'test_t15.2023.08.18', 'train_t15.2023.08.18', 'val_t15.2023.08.18', 'test_t15.2023.08.20', 'train_t15.2023.08.20', 'val_t15.2023.08.20', 'test_t15.2023.08.25', 'train_t15.2023.08.25', 'val_t15.2023.08.25', 'test_t15.2023.08.27', 'train_t15.2023.08.27', 'val_t15.2023.08.27', 'test_t15.2023.09.01', 'train_t15.2023.09.01', 'val_t15.2023.09.01', 'test_t15.2023.09.03', 'train_t15.2023.09.03', 'val_t15.2023.09.03', 'test_t15.2023.09.24', 'train_t15.2023.09.24', 'val_t15.2023.09.24', 'test_t15.2023.09.29', 'train_t15.2023.09.29', 'val_t15.2023.09.29', 'test_t15.2023.10.01', 'train_t15.2023.10.01', 'val_t15.2023.10.01', 'test_t15.2023.10.06', 'train_t15.2023.10.06', 'val_t15.2023.10.06', 'test_t15.2023.10.08', 'train_t15.2023.10.08', 'val_t15.2023.10.08', 'test_t15.2023.10.13', 'train_t15.2023.10.13', 'val_t15.2023.10.13', 'test_t15.2023.10.15', 'train_t15.2023.10.15', 'val_t15.2023.10.15', 'test_t15.2023.10.20', 'train_t15.2023.10.20', 'val_t15.2023.10.20', 'test_t15.2023.10.22', 'train_t15.2023.10.22', 'val_t15.2023.10.22', 'test_t15.2023.11.03', 'train_t15.2023.11.03', 'val_t15.2023.11.03', 'test_t15.2023.11.04', 'train_t15.2023.11.04', 'val_t15.2023.11.04', 'test_t15.2023.11.17', 'train_t15.2023.11.17', 'val_t15.2023.11.17', 'test_t15.2023.11.19', 'train_t15.2023.11.19', 'val_t15.2023.11.19', 'test_t15.2023.11.26', 'train_t15.2023.11.26', 'val_t15.2023.11.26', 'test_t15.2023.12.03', 'train_t15.2023.12.03', 'val_t15.2023.12.03', 'test_t15.2023.12.08', 'train_t15.2023.12.08', 'val_t15.2023.12.08', 'test_t15.2023.12.10', 'train_t15.2023.12.10', 'val_t15.2023.12.10', 'test_t15.2023.12.17', 'train_t15.2023.12.17', 'val_t15.2023.12.17', 'test_t15.2023.12.29', 'train_t15.2023.12.29', 'val_t15.2023.12.29', 'test_t15.2024.02.25', 'train_t15.2024.02.25', 'val_t15.2024.02.25', 'train_t15.2024.03.03', 'test_t15.2024.03.08', 'train_t15.2024.03.08', 'val_t15.2024.03.08', 'test_t15.2024.03.15', 'train_t15.2024.03.15', 'val_t15.2024.03.15', 'test_t15.2024.03.17', 'train_t15.2024.03.17', 'val_t15.2024.03.17', 'train_t15.2024.04.25', 'train_t15.2024.04.28', 'test_t15.2024.05.10', 'train_t15.2024.05.10', 'val_t15.2024.05.10', 'test_t15.2024.06.14', 'train_t15.2024.06.14', 'val_t15.2024.06.14', 'test_t15.2024.07.19', 'train_t15.2024.07.19', 'val_t15.2024.07.19', 'test_t15.2024.07.21', 'train_t15.2024.07.21', 'val_t15.2024.07.21', 'test_t15.2024.07.28', 'train_t15.2024.07.28', 'val_t15.2024.07.28', 'test_t15.2025.01.10', 'train_t15.2025.01.10', 'val_t15.2025.01.10', 'test_t15.2025.01.12', 'train_t15.2025.01.12', 'val_t15.2025.01.12', 'test_t15.2025.03.14', 'train_t15.2025.03.14', 'val_t15.2025.03.14', 'test_t15.2025.03.16', 'train_t15.2025.03.16', 'val_t15.2025.03.16', 'test_t15.2025.03.30', 'train_t15.2025.03.30', 'val_t15.2025.03.30', 'test_t15.2025.04.13', 'train_t15.2025.04.13', 'val_t15.2025.04.13'])\n"
     ]
    }
   ],
   "source": [
    "print(data_dict.keys())\n",
    "#test_df = pd.DataFrame.from_dict(data_dict['data_test.hdf5'])\n",
    "#train_df = pd.DataFrame.from_dict(data_dict['data_train.hdf5'])\n",
    "#val_df = pd.DataFrame.from_dict(data_dict['data_val.hdf5'])\n",
    "\n",
    "test_df = pd.DataFrame()\n",
    "train_df = pd.DataFrame()\n",
    "val_df = pd.DataFrame()\n",
    "\n",
    "for k in data_dict.keys():\n",
    "    if 'train' in k:\n",
    "        train_df = pd.concat([train_df, pd.DataFrame.from_dict(data_dict[k])], ignore_index=True)\n",
    "    elif 'val' in k:\n",
    "        val_df = pd.concat([val_df, pd.DataFrame.from_dict(data_dict[k])], ignore_index = True)\n",
    "    elif 'test' in k:\n",
    "        test_df = pd.concat([test_df, pd.DataFrame.from_dict(data_dict[k])], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(data_dict): 127\n",
      "first 10 keys: ['train_t15.2023.08.11', 'test_t15.2023.08.13', 'train_t15.2023.08.13', 'val_t15.2023.08.13', 'test_t15.2023.08.18', 'train_t15.2023.08.18', 'val_t15.2023.08.18', 'test_t15.2023.08.20', 'train_t15.2023.08.20', 'val_t15.2023.08.20']\n",
      "Train shape: (8072, 9)\n",
      "Val shape: (1426, 9)\n",
      "Test shape: (1450, 9)\n",
      "Sessions (train): 45\n",
      "Sessions (val): 41\n",
      "Sessions (test): 41\n"
     ]
    }
   ],
   "source": [
    "print(\"len(data_dict):\", len(data_dict))\n",
    "print(\"first 10 keys:\", list(data_dict.keys())[:10])\n",
    "\n",
    "print(\"Train shape:\", train_df.shape)\n",
    "print(\"Val shape:\",   val_df.shape)\n",
    "print(\"Test shape:\",  test_df.shape)\n",
    "\n",
    "print(\"Sessions (train):\", train_df['session'].nunique())\n",
    "print(\"Sessions (val):\",   val_df['session'].nunique())\n",
    "print(\"Sessions (test):\",  test_df['session'].nunique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-06T10:01:23.015260Z",
     "iopub.status.busy": "2025-11-06T10:01:23.014884Z",
     "iopub.status.idle": "2025-11-06T10:01:23.319440Z",
     "shell.execute_reply": "2025-11-06T10:01:23.318307Z",
     "shell.execute_reply.started": "2025-11-06T10:01:23.015214Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       n_time_steps      seq_len    block_num    trial_num\n",
      "count   8072.000000  8072.000000  8072.000000  8072.000000\n",
      "mean     874.840560    26.541625     4.500248    22.602081\n",
      "std      308.298035     9.154736     2.871122    14.403206\n",
      "min      138.000000     3.000000     1.000000     0.000000\n",
      "25%      655.000000    20.000000     2.000000    10.000000\n",
      "50%      836.000000    26.000000     4.000000    21.000000\n",
      "75%     1050.000000    33.000000     6.000000    35.000000\n",
      "max     2475.000000   110.000000    14.000000    49.000000\n",
      "       n_time_steps      seq_len    block_num    trial_num\n",
      "count   1426.000000  1426.000000  1426.000000  1426.000000\n",
      "mean     922.128331    29.026648     6.969144    11.048387\n",
      "std      318.502705     9.288702     2.416122     7.094434\n",
      "min      297.000000     8.000000     1.000000     0.000000\n",
      "25%      695.000000    22.000000     6.000000     5.000000\n",
      "50%      890.500000    29.000000     7.000000    11.000000\n",
      "75%     1094.000000    35.000000     9.000000    17.000000\n",
      "max     2382.000000    67.000000    13.000000    29.000000\n",
      "       n_time_steps    block_num    trial_num\n",
      "count   1450.000000  1450.000000  1450.000000\n",
      "mean     916.572414     6.972414    34.337241\n",
      "std      294.083394     2.408132     9.359402\n",
      "min      324.000000     1.000000     7.000000\n",
      "25%      692.250000     6.000000    28.000000\n",
      "50%      894.000000     7.000000    35.000000\n",
      "75%     1098.500000     9.000000    42.000000\n",
      "max     2140.000000    13.000000    59.000000\n",
      "Index(['neural_features', 'n_time_steps', 'seq_class_ids', 'seq_len',\n",
      "       'transcriptions', 'sentence_label', 'session', 'block_num',\n",
      "       'trial_num'],\n",
      "      dtype='object')\n",
      "Index(['neural_features', 'n_time_steps', 'seq_class_ids', 'seq_len',\n",
      "       'transcriptions', 'sentence_label', 'session', 'block_num',\n",
      "       'trial_num'],\n",
      "      dtype='object')\n",
      "                                     neural_features  n_time_steps  \\\n",
      "0  [[2.3076649, -0.78699756, -0.64687246, -0.5465...           321   \n",
      "1  [[-0.51709145, -0.70207363, -0.64330804, -0.48...           481   \n",
      "2  [[0.95464545, -0.6912571, 2.5334082, -0.459320...           480   \n",
      "3  [[-0.4997814, -0.6836047, -0.6305947, 1.260037...           502   \n",
      "4  [[-0.4850082, -0.66607094, -0.62398034, -0.433...           402   \n",
      "\n",
      "                                       seq_class_ids  seq_len  \\\n",
      "0  [7, 28, 17, 24, 40, 17, 31, 40, 20, 21, 25, 29...       14   \n",
      "1  [22, 6, 40, 14, 2, 22, 3, 21, 18, 40, 17, 38, ...       19   \n",
      "2  [36, 3, 31, 40, 9, 34, 40, 10, 13, 40, 21, 6, ...       14   \n",
      "3  [16, 5, 40, 17, 38, 40, 10, 2, 31, 40, 15, 33,...       14   \n",
      "4  [23, 18, 9, 40, 16, 11, 21, 27, 40, 16, 18, 28...       13   \n",
      "\n",
      "                                      transcriptions        sentence_label  \\\n",
      "0  [66, 114, 105, 110, 103, 32, 105, 116, 32, 99,...      Bring it closer.   \n",
      "1  [77, 121, 32, 102, 97, 109, 105, 108, 121, 32,...  My family is closer.   \n",
      "2  [87, 104, 97, 116, 32, 100, 111, 32, 116, 104,...    What do they like?   \n",
      "3  [72, 111, 119, 32, 105, 115, 32, 116, 104, 97,...     How is that good?   \n",
      "4  [78, 101, 101, 100, 32, 104, 101, 108, 112, 32...       Need help here?   \n",
      "\n",
      "          session  block_num  trial_num  \n",
      "0  t15.2023.08.11          2          0  \n",
      "1  t15.2023.08.11          2          1  \n",
      "2  t15.2023.08.11          2          2  \n",
      "3  t15.2023.08.11          2          3  \n",
      "4  t15.2023.08.11          2          4  \n",
      "_______TRAIN_______\n",
      "n_time_steps\n",
      "833     24\n",
      "696     23\n",
      "931     21\n",
      "932     20\n",
      "150     20\n",
      "        ..\n",
      "472      1\n",
      "1296     1\n",
      "1505     1\n",
      "1554     1\n",
      "1670     1\n",
      "Name: count, Length: 1392, dtype: int64\n",
      "1392\n",
      "_____VALIDATION_____\n",
      "n_time_steps\n",
      "743     7\n",
      "803     6\n",
      "910     5\n",
      "769     5\n",
      "963     5\n",
      "       ..\n",
      "536     1\n",
      "785     1\n",
      "728     1\n",
      "562     1\n",
      "1142    1\n",
      "Name: count, Length: 809, dtype: int64\n",
      "809\n",
      "_______TEST_______\n",
      "n_time_steps\n",
      "845     6\n",
      "1109    6\n",
      "710     6\n",
      "873     6\n",
      "1077    6\n",
      "       ..\n",
      "1175    1\n",
      "462     1\n",
      "1444    1\n",
      "684     1\n",
      "1030    1\n",
      "Name: count, Length: 798, dtype: int64\n",
      "798\n",
      "_______TRAIN_______\n",
      "seq_len\n",
      "21    359\n",
      "22    341\n",
      "25    328\n",
      "27    323\n",
      "24    319\n",
      "     ... \n",
      "56      1\n",
      "80      1\n",
      "69      1\n",
      "61      1\n",
      "64      1\n",
      "Name: count, Length: 67, dtype: int64\n",
      "67\n",
      "_____VALIDATION_____\n",
      "seq_len\n",
      "29    72\n",
      "25    64\n",
      "26    63\n",
      "28    63\n",
      "22    62\n",
      "36    57\n",
      "32    56\n",
      "35    55\n",
      "34    54\n",
      "30    52\n",
      "33    51\n",
      "21    48\n",
      "24    48\n",
      "31    46\n",
      "20    46\n",
      "27    46\n",
      "23    44\n",
      "19    42\n",
      "18    42\n",
      "39    40\n",
      "17    34\n",
      "16    33\n",
      "40    31\n",
      "37    31\n",
      "15    31\n",
      "38    29\n",
      "41    27\n",
      "14    22\n",
      "43    21\n",
      "42    18\n",
      "13    11\n",
      "11     8\n",
      "48     7\n",
      "44     7\n",
      "46     7\n",
      "45     6\n",
      "47     6\n",
      "49     5\n",
      "50     5\n",
      "60     4\n",
      "52     4\n",
      "59     4\n",
      "56     3\n",
      "62     3\n",
      "54     3\n",
      "58     3\n",
      "57     2\n",
      "12     2\n",
      "55     1\n",
      "10     1\n",
      "9      1\n",
      "51     1\n",
      "65     1\n",
      "67     1\n",
      "61     1\n",
      "8      1\n",
      "Name: count, dtype: int64\n",
      "56\n",
      "_______TEST_______\n",
      "Series([], Name: count, dtype: int64)\n",
      "0\n",
      "_______TRAIN_______\n",
      "session\n",
      "t15.2024.04.25    364\n",
      "t15.2023.08.13    348\n",
      "t15.2023.09.03    322\n",
      "t15.2023.09.01    297\n",
      "t15.2023.08.11    288\n",
      "t15.2023.10.08    284\n",
      "t15.2023.08.20    278\n",
      "t15.2024.03.17    246\n",
      "t15.2023.09.24    245\n",
      "t15.2024.03.15    239\n",
      "t15.2023.10.15    239\n",
      "t15.2023.12.03    228\n",
      "t15.2024.03.03    219\n",
      "t15.2023.10.01    218\n",
      "t15.2023.11.26    198\n",
      "t15.2023.12.29    198\n",
      "t15.2023.12.08    198\n",
      "t15.2023.08.18    197\n",
      "t15.2024.02.25    193\n",
      "t15.2023.10.06    174\n",
      "t15.2024.07.19    169\n",
      "t15.2025.03.30    165\n",
      "t15.2025.01.12    163\n",
      "t15.2024.03.08    163\n",
      "t15.2024.07.28    161\n",
      "t15.2024.07.21    160\n",
      "t15.2023.10.13    155\n",
      "t15.2023.09.29    153\n",
      "t15.2023.08.27    150\n",
      "t15.2024.04.28    150\n",
      "t15.2023.11.03    149\n",
      "t15.2023.12.17    135\n",
      "t15.2023.10.22    134\n",
      "t15.2023.12.10    131\n",
      "t15.2024.05.10    110\n",
      "t15.2025.01.10    106\n",
      "t15.2025.03.16    101\n",
      "t15.2023.11.17    100\n",
      "t15.2023.10.20     98\n",
      "t15.2024.06.14     90\n",
      "t15.2023.08.25     88\n",
      "t15.2023.11.04     80\n",
      "t15.2025.04.13     69\n",
      "t15.2023.11.19     60\n",
      "t15.2025.03.14     59\n",
      "Name: count, dtype: int64\n",
      "45\n",
      "_____VALIDATION_____\n",
      "session\n",
      "t15.2023.12.08    50\n",
      "t15.2023.11.03    50\n",
      "t15.2023.12.29    50\n",
      "t15.2023.09.01    49\n",
      "t15.2023.08.18    49\n",
      "t15.2024.03.15    48\n",
      "t15.2023.08.20    48\n",
      "t15.2024.07.28    48\n",
      "t15.2024.07.19    48\n",
      "t15.2023.09.29    48\n",
      "t15.2024.03.17    48\n",
      "t15.2025.01.12    47\n",
      "t15.2024.07.21    46\n",
      "t15.2023.11.26    44\n",
      "t15.2023.10.15    44\n",
      "t15.2023.10.13    44\n",
      "t15.2023.10.01    44\n",
      "t15.2023.10.06    36\n",
      "t15.2023.08.13    35\n",
      "t15.2023.09.24    35\n",
      "t15.2023.12.03    34\n",
      "t15.2023.09.03    34\n",
      "t15.2023.10.22    33\n",
      "t15.2025.03.30    30\n",
      "t15.2023.12.17    30\n",
      "t15.2024.05.10    25\n",
      "t15.2023.11.17    25\n",
      "t15.2023.08.25    25\n",
      "t15.2023.08.27    25\n",
      "t15.2024.06.14    25\n",
      "t15.2025.04.13    25\n",
      "t15.2023.12.10    25\n",
      "t15.2024.03.08    24\n",
      "t15.2025.03.14    24\n",
      "t15.2025.03.16    24\n",
      "t15.2024.02.25    23\n",
      "t15.2025.01.10    23\n",
      "t15.2023.11.19    20\n",
      "t15.2023.10.08    17\n",
      "t15.2023.11.04    15\n",
      "t15.2023.10.20     9\n",
      "Name: count, dtype: int64\n",
      "41\n",
      "_______TEST_______\n",
      "session\n",
      "t15.2023.12.08    50\n",
      "t15.2024.03.15    50\n",
      "t15.2023.09.01    50\n",
      "t15.2023.08.18    50\n",
      "t15.2023.11.03    50\n",
      "t15.2023.12.29    50\n",
      "t15.2023.08.20    49\n",
      "t15.2024.07.28    49\n",
      "t15.2024.07.19    49\n",
      "t15.2023.09.29    49\n",
      "t15.2024.03.17    49\n",
      "t15.2025.01.12    47\n",
      "t15.2024.07.21    47\n",
      "t15.2023.11.26    45\n",
      "t15.2023.10.15    45\n",
      "t15.2023.10.13    45\n",
      "t15.2023.10.01    45\n",
      "t15.2023.10.06    37\n",
      "t15.2023.08.13    35\n",
      "t15.2023.10.22    35\n",
      "t15.2023.09.03    35\n",
      "t15.2023.09.24    35\n",
      "t15.2023.12.03    34\n",
      "t15.2025.03.30    30\n",
      "t15.2023.12.17    30\n",
      "t15.2025.03.14    26\n",
      "t15.2024.05.10    25\n",
      "t15.2024.06.14    25\n",
      "t15.2023.08.25    25\n",
      "t15.2023.08.27    25\n",
      "t15.2025.04.13    25\n",
      "t15.2023.11.17    25\n",
      "t15.2024.03.08    25\n",
      "t15.2023.12.10    25\n",
      "t15.2025.01.10    24\n",
      "t15.2024.02.25    24\n",
      "t15.2025.03.16    24\n",
      "t15.2023.11.19    20\n",
      "t15.2023.10.08    18\n",
      "t15.2023.11.04    15\n",
      "t15.2023.10.20     9\n",
      "Name: count, dtype: int64\n",
      "41\n",
      "_______TRAIN_______\n",
      "block_num\n",
      "1     1308\n",
      "2     1147\n",
      "5     1019\n",
      "3      997\n",
      "6      913\n",
      "4      894\n",
      "7      735\n",
      "8      367\n",
      "9      194\n",
      "11     169\n",
      "12     129\n",
      "10     100\n",
      "13      50\n",
      "14      50\n",
      "Name: count, dtype: int64\n",
      "14\n",
      "_____VALIDATION_____\n",
      "block_num\n",
      "7     282\n",
      "8     230\n",
      "9     218\n",
      "6     213\n",
      "4     119\n",
      "5     109\n",
      "3      59\n",
      "1      50\n",
      "10     50\n",
      "11     40\n",
      "12     32\n",
      "13     24\n",
      "Name: count, dtype: int64\n",
      "12\n",
      "_______TEST_______\n",
      "block_num\n",
      "7     288\n",
      "8     233\n",
      "9     222\n",
      "6     218\n",
      "4     121\n",
      "5     111\n",
      "3      59\n",
      "10     51\n",
      "1      50\n",
      "11     40\n",
      "12     33\n",
      "13     24\n",
      "Name: count, dtype: int64\n",
      "12\n",
      "_______TRAIN_______\n",
      "trial_num\n",
      "3     196\n",
      "2     194\n",
      "1     194\n",
      "0     193\n",
      "4     193\n",
      "6     193\n",
      "9     193\n",
      "7     192\n",
      "8     192\n",
      "5     189\n",
      "11    185\n",
      "14    184\n",
      "12    184\n",
      "10    183\n",
      "13    182\n",
      "15    176\n",
      "16    176\n",
      "18    176\n",
      "17    174\n",
      "19    172\n",
      "20    162\n",
      "21    160\n",
      "22    159\n",
      "23    159\n",
      "24    157\n",
      "28    154\n",
      "27    151\n",
      "25    150\n",
      "26    150\n",
      "29    148\n",
      "34    148\n",
      "32    147\n",
      "33    147\n",
      "31    146\n",
      "35    146\n",
      "38    145\n",
      "39    145\n",
      "37    144\n",
      "36    144\n",
      "30    144\n",
      "41    136\n",
      "42    136\n",
      "43    136\n",
      "44    136\n",
      "45    135\n",
      "48    135\n",
      "49    134\n",
      "46    133\n",
      "40    132\n",
      "47    132\n",
      "Name: count, dtype: int64\n",
      "50\n",
      "_____VALIDATION_____\n",
      "trial_num\n",
      "1     67\n",
      "2     66\n",
      "3     66\n",
      "4     66\n",
      "5     66\n",
      "6     66\n",
      "8     65\n",
      "0     64\n",
      "7     64\n",
      "9     63\n",
      "10    59\n",
      "11    58\n",
      "13    57\n",
      "14    56\n",
      "12    55\n",
      "19    54\n",
      "18    54\n",
      "16    53\n",
      "17    53\n",
      "15    51\n",
      "20    47\n",
      "21    46\n",
      "22    46\n",
      "23    43\n",
      "24    34\n",
      "25     3\n",
      "26     1\n",
      "27     1\n",
      "28     1\n",
      "29     1\n",
      "Name: count, dtype: int64\n",
      "30\n",
      "_______TEST_______\n",
      "trial_num\n",
      "26    57\n",
      "29    57\n",
      "28    56\n",
      "34    53\n",
      "36    53\n",
      "39    53\n",
      "38    53\n",
      "25    53\n",
      "27    53\n",
      "31    53\n",
      "32    53\n",
      "33    53\n",
      "30    52\n",
      "37    52\n",
      "35    52\n",
      "41    48\n",
      "42    48\n",
      "44    47\n",
      "45    46\n",
      "43    46\n",
      "40    45\n",
      "49    45\n",
      "48    45\n",
      "47    45\n",
      "46    45\n",
      "24    24\n",
      "23    15\n",
      "22    13\n",
      "21    12\n",
      "20    11\n",
      "18    11\n",
      "17    11\n",
      "15    11\n",
      "19    10\n",
      "16    10\n",
      "13    10\n",
      "14    10\n",
      "12     9\n",
      "11     7\n",
      "10     6\n",
      "9      4\n",
      "7      2\n",
      "8      1\n",
      "50     1\n",
      "51     1\n",
      "52     1\n",
      "53     1\n",
      "54     1\n",
      "55     1\n",
      "56     1\n",
      "57     1\n",
      "58     1\n",
      "59     1\n",
      "Name: count, dtype: int64\n",
      "53\n"
     ]
    }
   ],
   "source": [
    "print(train_df.describe())\n",
    "print(val_df.describe())\n",
    "print(test_df.describe())\n",
    "print(train_df.columns)\n",
    "print(val_df.columns)\n",
    "#512 neural features (2 features [-4.5 RMS threshold crossings and spike band power] per electrode, \n",
    "#256 electrodes), binned at 20 ms resolution. The data were recorded from the speech motor cortex \n",
    "#via four high-density microelectrode arrays (64 electrodes each). The 512 features are ordered \n",
    "#as follows in all data files:\n",
    "#0-64: ventral 6v threshold crossings\n",
    "#65-128: area 4 threshold crossings\n",
    "#129-192: 55b threshold crossings\n",
    "#193-256: dorsal 6v threshold crossings\n",
    "#257-320: ventral 6v spike band power\n",
    "#321-384: area 4 spike band power\n",
    "#385-448: 55b spike band power\n",
    "#449-512: dorsal 6v spike band power\n",
    "#for i in range(train_df['neural_features'].shape[0]):\n",
    "    #print(train_df['neural_features'][i].shape) #trialx(n)x512, with n = n_time_steps\n",
    "print(train_df.head())\n",
    "for c in ['n_time_steps', 'seq_len', 'session', 'block_num', 'trial_num']:\n",
    "    print(\"_______TRAIN_______\")\n",
    "    print(train_df[c].value_counts())\n",
    "    print(train_df[c].nunique())\n",
    "    print(\"_____VALIDATION_____\")\n",
    "    print(val_df[c].value_counts())\n",
    "    print(val_df[c].nunique())\n",
    "    print(\"_______TEST_______\")\n",
    "    print(test_df[c].value_counts())\n",
    "    print(test_df[c].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-06T10:01:32.470519Z",
     "iopub.status.busy": "2025-11-06T10:01:32.470173Z",
     "iopub.status.idle": "2025-11-06T10:01:32.483640Z",
     "shell.execute_reply": "2025-11-06T10:01:32.482251Z",
     "shell.execute_reply.started": "2025-11-06T10:01:32.470498Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HH\n",
      "AW\n",
      " | \n",
      "IH\n",
      "Z\n",
      " | \n",
      "DH\n",
      "AE\n",
      "T\n",
      " | \n",
      "G\n",
      "UH\n",
      "D\n",
      " | \n",
      "[16  5 40 17 38 40 10  2 31 40 15 33  9 40]\n",
      "How is that good?\n",
      "[ 72 111 119  32 105 115  32 116 104  97 116  32 103 111]\n",
      "H\n",
      "o\n",
      "w\n",
      " \n",
      "i\n",
      "s\n",
      " \n",
      "t\n",
      "h\n",
      "a\n",
      "t\n",
      " \n",
      "g\n",
      "o\n",
      "W\n",
      "AH\n",
      "T\n",
      " | \n",
      "D\n",
      "UW\n",
      " | \n",
      "DH\n",
      "EY\n",
      " | \n",
      "L\n",
      "AY\n",
      "K\n",
      " | \n",
      "[36  3 31 40  9 34 40 10 13 40 21  6 20 40]\n",
      "What do they like?\n",
      "[ 87 104  97 116  32 100 111  32 116 104 101 121  32 108]\n",
      "W\n",
      "h\n",
      "a\n",
      "t\n",
      " \n",
      "d\n",
      "o\n",
      " \n",
      "t\n",
      "h\n",
      "e\n",
      "y\n",
      " \n",
      "l\n"
     ]
    }
   ],
   "source": [
    "for i in train_df['seq_class_ids'][3][:train_df['seq_len'][3]]:\n",
    "    print(LOGIT_TO_PHONEME[i])\n",
    "print(train_df['seq_class_ids'][3][:train_df['seq_len'][3]])\n",
    "print(train_df['sentence_label'][3])\n",
    "print(train_df['transcriptions'][3][:train_df['seq_len'][3]])\n",
    "for i in train_df['transcriptions'][3][:train_df['seq_len'][3]]:\n",
    "    print(chr(i))\n",
    "for i in train_df['seq_class_ids'][2][:train_df['seq_len'][2]]:\n",
    "    print(LOGIT_TO_PHONEME[i])\n",
    "print(train_df['seq_class_ids'][2][:train_df['seq_len'][2]])\n",
    "print(train_df['sentence_label'][2])\n",
    "print(train_df['transcriptions'][2][:train_df['seq_len'][2]])\n",
    "for i in train_df['transcriptions'][2][:train_df['seq_len'][2]]:\n",
    "    print(chr(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-06T10:01:36.328639Z",
     "iopub.status.busy": "2025-11-06T10:01:36.328257Z",
     "iopub.status.idle": "2025-11-06T10:01:36.344949Z",
     "shell.execute_reply": "2025-11-06T10:01:36.343643Z",
     "shell.execute_reply.started": "2025-11-06T10:01:36.328611Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2.3076649  -0.78699756 -0.64687246 -0.5465877   0.25500455 -0.37754795\n",
      " -0.31888878 -0.43742913 -0.552158   -0.6198629  -0.2722918   1.0336585\n",
      "  0.27755055 -0.666718   -0.6310771  -0.78702307 -0.59295505  0.8399486\n",
      "  0.85698867  0.34778863 -0.7790861  -0.843797    0.7688951  -0.77096075\n",
      " -0.437959    2.4420617   0.29788262 -0.60283345 -0.4174294  -0.5666092\n",
      " -0.7893149   0.16854596 -0.54237986 -0.3172244  -0.7257092  -0.6197359\n",
      "  0.15344943 -0.38841027 -0.76966536 -0.27819777  0.53903705 -0.6611451\n",
      " -0.75468504 -0.6057648  -0.73880273 -0.4290835  -0.94724435  0.6137742\n",
      " -0.5463916  -0.41273063 -0.8792733   0.97371936  1.0565114  -0.5758789\n",
      "  2.4920397  -0.42387414 -0.22300059 -0.4641142  -0.5492305  -0.46833596\n",
      "  3.2247906  -0.11923911  1.5838451  -0.09712851 -0.7942411  -0.5100695\n",
      " -0.68580496 -0.5852925  -0.603436    0.50690377 -0.30034852  4.1031313\n",
      " -0.6948983  -0.5562878   1.5018272  -0.35887727 -0.5901224  -0.63912416\n",
      " -0.52937365 -0.16325521  0.74647266 -0.13449022 -0.15371595 -0.8086595\n",
      " -0.46007815 -0.39804113  0.63136655 -0.5603162   0.66564244  1.026342\n",
      "  1.0497364  -0.5676086   1.9144232  -0.16984162 -0.69077396 -0.2223696\n",
      " -0.45084962  0.31810564 -1.2886951   2.8417828   1.9198556  -0.65563273\n",
      " -0.8808405  -0.5839129  -0.21928343 -0.93472826 -1.1244689   0.1570389\n",
      "  0.25946248 -0.7659722  -0.6609633  -0.40443888 -0.28115866 -0.6968133\n",
      " -1.0455226  -0.9092157   0.80293995  0.6022096  -1.072319    0.4672438\n",
      "  3.2495701   4.322763   -0.4374437   0.20416275  2.4733093  -0.4016732\n",
      " -0.28518087 -0.3647703  -1.0060111  -1.1173934   0.7696486  -0.9979249\n",
      " -0.4254152   2.2359376  -0.32826716 -0.11233114 -0.73417306 -0.87693644\n",
      " -0.13625346 -0.82149994  1.1091884  -0.516467   -0.6061519  -0.81583285\n",
      " -0.6630439   1.6290879  -0.91519433 -0.9548456  -0.7729239   0.7957438\n",
      " -1.1268758  -0.8204315  -0.95808965 -0.5102313  -0.61390674  0.22336504\n",
      " -0.6789398  -0.57929665 -0.87530965 -0.8403082  -1.3545448  -0.8724641\n",
      " -0.3820907   1.8482778   0.28298536 -0.7547832  -0.5477684  -0.8110415\n",
      " -0.07918064 -0.77779204 -0.8993781   0.05489132 -0.10873149 -0.5942442\n",
      " -0.6294594  -0.6559725  -0.6489255  -0.9875131   1.0031364  -0.8591877\n",
      "  0.6224035  -0.07454145  0.88708365 -0.14930865 -0.49931344 -0.5340475\n",
      "  4.0510144  -0.9193805   0.5350056  -0.61930287 -0.8403575  -1.0220783\n",
      " -0.4562829   0.6074758  -0.46520656 -0.2654165  -0.5293411   0.77331173\n",
      " -0.5509832  -0.6326867  -0.2547191  -0.66311663  0.9357117   0.5606107\n",
      " -0.5834567  -0.63425255  0.62541795 -0.62835765 -0.61680925 -0.6744631\n",
      "  0.4358965  -0.8852247  -0.9182702   0.31465283 -1.0237244  -1.0429336\n",
      " -1.0892158  -0.89635646 -0.93282217 -0.7034446  -0.73058265 -0.17608659\n",
      " -0.9861013  -0.571891    1.1369307   0.73664635  0.74893177  0.58920074\n",
      " -0.6642993   2.009673    1.6065629  -0.71492577 -0.5425989  -0.8438985\n",
      " -0.6304679   1.6823055   1.3655643  -0.653197   -0.47072423  1.2801076\n",
      " -0.6915899  -0.78682315 -0.4016736   0.07206184  0.48307127  2.4204607\n",
      " -0.20544502 -0.53256726 -0.56156164 -0.75127965 -1.0972689  -0.8764349\n",
      " -0.48201936  0.83810395 -0.3289254  -0.3731014   0.42762992 -0.9927089\n",
      " -0.98290706 -1.3720226  -0.7368366  -0.502273   -0.38284197 -0.43514246\n",
      " -0.45898786 -0.81552994 -1.037468    1.5269672   0.24386652 -0.4810984\n",
      " -0.73050463 -1.0578549  -1.0728137   0.24289073  0.42365366  1.1153152\n",
      " -0.96126634 -0.76943684  1.0387434  -0.61462307 -0.493274   -0.00854964\n",
      " -0.49241474 -0.70684296 -0.4668919  -0.95333564 -0.9210525   1.2389431\n",
      " -0.4261885  -0.535416   -1.151569   -0.14319889  0.2239445  -0.42169595\n",
      " -0.4532525  -0.6107523  -0.43080604 -0.7506939  -0.77451265 -0.5004188\n",
      " -0.90239763 -0.24248217 -0.71451616  0.25715673 -0.37129506 -0.9613883\n",
      " -1.056714   -0.51006186  0.09554572 -0.9755061  -0.23866284 -0.87683046\n",
      " -0.78057927 -0.7295193  -0.8654158  -0.6977953   2.3287816  -1.1569148\n",
      "  1.6476707  -1.5092787  -0.21502116 -0.48730668 -0.7174389  -0.6698602\n",
      " -0.6829336  -0.23655677  0.08789751  1.719456   -1.2945591  -1.2484773\n",
      "  0.3577217  -0.79188746 -0.63958734 -0.9469683  -0.6300207  -0.50196713\n",
      " -0.55603164 -0.7137818   0.05313407 -1.0117475  -1.3233516  -1.3446937\n",
      "  0.05052676 -1.273296   -0.02342449  0.04944937  0.11576715 -0.34752378\n",
      " -0.10363815 -0.78256994 -1.2645344  -0.88083106  0.10641053  0.6660232\n",
      " -1.4196591   1.1814      0.6185777  -0.81786364 -0.89482    -0.3712795\n",
      " -1.3915757  -1.0902308  -1.3552573  -0.35811502  0.20781317 -0.7562471\n",
      " -1.0844029  -0.344378   -0.33516216 -0.5914646  -1.1701252  -0.8121005\n",
      "  0.46121246  0.12646843 -1.2247303   0.1996745   1.9819672   2.9660134\n",
      " -0.91374826  0.11389273  2.9381824  -0.96545607 -1.190995    0.2114223\n",
      " -1.2452503  -1.6840367   0.34562877 -0.6625789  -1.3360639  -0.5963416\n",
      " -0.94754106 -1.6605002  -1.6370404  -1.1194495   0.19468893 -0.99948835\n",
      "  0.21448335 -0.87200075 -0.78745717 -1.0645713  -0.77728426  0.93718135\n",
      " -1.174949   -1.275437   -1.1333019  -0.43346545 -1.0911902  -0.56418794\n",
      " -0.8720056  -1.1467232  -1.0167987   0.02811413 -1.2235795  -0.78109264\n",
      " -0.629308   -1.1361895  -1.4013705  -1.0130705  -0.438414    1.0221207\n",
      " -0.28523642 -0.6728531  -0.9068811  -0.71865153 -0.80234414 -0.889773\n",
      " -0.6783438  -0.36688447 -0.47375116 -0.22822705 -0.9266897  -0.918847\n",
      "  0.9255994  -0.99864227  0.6519056  -0.84216774 -0.33204818  0.84402657\n",
      "  1.4956689   0.53440595 -0.90524226 -0.6636251   2.5546823  -1.2869236\n",
      " -0.17336662 -0.5226231  -0.8251883  -1.0195179  -0.47750133  1.3557949\n",
      " -0.5933006   0.9620423   0.1947498   0.9728769  -0.24200271 -0.4706283\n",
      " -0.23185177 -0.8421882   1.2021793  -0.3844243  -0.78168905 -0.72828543\n",
      "  0.6498486  -0.70523095 -0.5805256  -0.26650724 -0.01899726 -0.2409702\n",
      " -1.0387025   0.642037   -0.8106473  -0.8860233  -0.80346626 -1.5534935\n",
      " -0.604658    0.18168718 -1.0749094   0.14102179 -0.92183375 -0.48595896\n",
      "  1.7163332   1.1171715  -0.09714307  0.8041192  -0.21881002  2.0348198\n",
      "  0.981629   -0.00735747 -0.66939825 -1.0059066   0.0183885   1.836036\n",
      "  1.0928419  -0.57912314 -0.02769383  0.05764658 -1.0663698  -0.8708504\n",
      " -0.20133011  0.24667199 -0.16636291  2.6357744  -0.33262774  0.0069312\n",
      " -0.7148367  -1.0180258  -0.88465786 -0.96628463 -0.7634382   0.57367045\n",
      " -0.7091646  -0.11018186]\n",
      "0        6.42\n",
      "1        9.62\n",
      "2        9.60\n",
      "3       10.04\n",
      "4        8.04\n",
      "        ...  \n",
      "8067    19.94\n",
      "8068    14.30\n",
      "8069    17.20\n",
      "8070    16.04\n",
      "8071    13.80\n",
      "Name: n_time_steps, Length: 8072, dtype: float64\n",
      "neural_features\n",
      "object\n",
      "n_time_steps\n",
      "int64\n",
      "seq_class_ids\n",
      "object\n",
      "seq_len\n",
      "int64\n",
      "transcriptions\n",
      "object\n",
      "sentence_label\n",
      "object\n",
      "session\n",
      "object\n",
      "block_num\n",
      "int64\n",
      "trial_num\n",
      "int64\n"
     ]
    }
   ],
   "source": [
    "print(train_df['neural_features'][0][0][:])\n",
    "print(train_df['n_time_steps'][:]*.02)\n",
    "for c in train_df.columns:\n",
    "    print(c)\n",
    "    print(train_df[c].dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-06T10:01:40.057368Z",
     "iopub.status.busy": "2025-11-06T10:01:40.056943Z",
     "iopub.status.idle": "2025-11-06T10:01:40.063443Z",
     "shell.execute_reply": "2025-11-06T10:01:40.061982Z",
     "shell.execute_reply.started": "2025-11-06T10:01:40.057340Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(500,)\n",
      "(500,)\n"
     ]
    }
   ],
   "source": [
    "print(train_df['transcriptions'][0].shape)\n",
    "print(train_df['seq_class_ids'][0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time to get dirty ;)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "NUM_CLASSES: 41\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "EXPERIMENT_NAME = \"bilstm_large_sched\"\n",
    "CKPT_PATH = f\"rnn_ctc_best_{EXPERIMENT_NAME}.pth\"\n",
    "\n",
    "# ===== Basic config =====\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "FEAT_DIM = 512                          # 512 neural features\n",
    "BLANK_ID = 0                            # 'BLANK' is first in LOGIT_TO_PHONEME\n",
    "NUM_CLASSES = len(LOGIT_TO_PHONEME)     # include BLANK\n",
    "\n",
    "BATCH_SIZE   = 16\n",
    "EPOCHS       = 30\n",
    "BASE_LR      = 5e-3\n",
    "WEIGHT_DECAY = 1e-4\n",
    "PATIENCE     = 5                        # early stopping on val PER\n",
    "\n",
    "print(\"Device:\", DEVICE)\n",
    "print(\"NUM_CLASSES:\", NUM_CLASSES)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "\n",
    "class BrainCTCDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Wraps train_df / val_df.\n",
    "    \n",
    "    Expected columns:\n",
    "      - 'neural_features': (T, 512) numpy array\n",
    "      - 'n_time_steps'   : int T\n",
    "      - 'seq_class_ids'  : array/list of phoneme IDs\n",
    "      - 'seq_len'        : int L (valid target length)\n",
    "    \"\"\"\n",
    "    def __init__(self, df):\n",
    "        df = df.copy()\n",
    "        # keep only rows with labels\n",
    "        df = df[df[\"seq_len\"].notnull()]\n",
    "        self.df = df.reset_index(drop=True)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "\n",
    "        feats = row[\"neural_features\"]        # (T, 512)\n",
    "        T = int(row[\"n_time_steps\"])\n",
    "        L = int(row[\"seq_len\"])\n",
    "\n",
    "        # convert to tensor\n",
    "        if isinstance(feats, np.ndarray):\n",
    "            feats = torch.from_numpy(feats).float()\n",
    "        else:\n",
    "            feats = feats.float()\n",
    "\n",
    "        assert feats.shape[0] == T\n",
    "        assert feats.shape[1] == FEAT_DIM\n",
    "\n",
    "        seq_ids = row[\"seq_class_ids\"]\n",
    "        labels = torch.as_tensor(seq_ids[:L], dtype=torch.long)\n",
    "\n",
    "        sample = {\n",
    "            \"feats\": feats,      # (T, 512)\n",
    "            \"T\": T,\n",
    "            \"labels\": labels,    # (L,)\n",
    "            \"L\": L,\n",
    "        }\n",
    "        return sample\n",
    "\n",
    "\n",
    "def ctc_collate_fn(batch):\n",
    "    \"\"\"\n",
    "    Pads inputs and concatenates labels for CTC.\n",
    "    Returns:\n",
    "      padded_feats: (B, max_T, 512)\n",
    "      input_lengths: (B,)\n",
    "      flat_labels: (sum_L,)\n",
    "      label_lengths: (B,)\n",
    "    \"\"\"\n",
    "    B = len(batch)\n",
    "    T_list = [b[\"T\"] for b in batch]\n",
    "    L_list = [b[\"L\"] for b in batch]\n",
    "    max_T = max(T_list)\n",
    "\n",
    "    padded_feats = torch.zeros(B, max_T, FEAT_DIM, dtype=torch.float32)\n",
    "    flat_labels_list = []\n",
    "\n",
    "    for i, b in enumerate(batch):\n",
    "        T = b[\"T\"]\n",
    "        padded_feats[i, :T] = b[\"feats\"]\n",
    "        if b[\"L\"] > 0:\n",
    "            flat_labels_list.append(b[\"labels\"])\n",
    "\n",
    "    if flat_labels_list:\n",
    "        flat_labels = torch.cat(flat_labels_list, dim=0)\n",
    "    else:\n",
    "        flat_labels = torch.zeros(0, dtype=torch.long)\n",
    "\n",
    "    input_lengths = torch.as_tensor(T_list, dtype=torch.long)\n",
    "    label_lengths = torch.as_tensor(L_list, dtype=torch.long)\n",
    "\n",
    "    return padded_feats, input_lengths, flat_labels, label_lengths\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(train_ds) = 8072\n",
      "len(val_ds)   = 1426\n",
      "<class 'torch.Tensor'> torch.Size([16, 948, 512])\n",
      "<class 'torch.Tensor'> torch.Size([16])\n",
      "<class 'torch.Tensor'> torch.Size([381])\n",
      "<class 'torch.Tensor'> torch.Size([16])\n"
     ]
    }
   ],
   "source": [
    "train_ds = BrainCTCDataset(train_df)\n",
    "val_ds   = BrainCTCDataset(val_df)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_ds,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    collate_fn=ctc_collate_fn,\n",
    "    num_workers=0,\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_ds,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    collate_fn=ctc_collate_fn,\n",
    "    num_workers=0,\n",
    ")\n",
    "\n",
    "print(\"len(train_ds) =\", len(train_ds))\n",
    "print(\"len(val_ds)   =\", len(val_ds))\n",
    "\n",
    "# quick sanity check: get one batch\n",
    "batch = next(iter(train_loader))\n",
    "for x in batch:\n",
    "    print(type(x), getattr(x, \"shape\", None))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNNCTCEncoder(\n",
      "  (lstm): LSTM(512, 384, num_layers=3, dropout=0.25, bidirectional=True)\n",
      "  (proj): Linear(in_features=768, out_features=41, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class RNNCTCEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Larger BiLSTM encoder + linear CTC head.\n",
    "    \"\"\"\n",
    "    def __init__(self, feat_dim=FEAT_DIM, hidden=384, num_layers=3, dropout=0.25):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=feat_dim,\n",
    "            hidden_size=hidden,\n",
    "            num_layers=num_layers,\n",
    "            dropout=dropout if num_layers > 1 else 0.0,\n",
    "            bidirectional=True,\n",
    "        )\n",
    "        self.proj = nn.Linear(2 * hidden, NUM_CLASSES)\n",
    "\n",
    "    def forward(self, x, input_lengths):\n",
    "        out, _ = self.lstm(x)      # (T, B, 2*hidden)\n",
    "        logits = self.proj(out)    # (T, B, C)\n",
    "        return logits              # lengths unchanged\n",
    "\n",
    "\n",
    "model = RNNCTCEncoder().to(DEVICE)\n",
    "criterion = nn.CTCLoss(blank=BLANK_ID, zero_infinity=True)\n",
    "optimizer = torch.optim.AdamW(\n",
    "    model.parameters(),\n",
    "    lr=BASE_LR,\n",
    "    weight_decay=WEIGHT_DECAY,\n",
    ")\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer,\n",
    "    mode=\"min\",\n",
    "    factor=0.5,\n",
    "    patience=2,\n",
    ")\n",
    "\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_decode(log_probs, input_lengths, blank_id=BLANK_ID):\n",
    "    \"\"\"\n",
    "    log_probs: (T, B, C)\n",
    "    input_lengths: (B,)\n",
    "    Returns: list of list[int], length B\n",
    "    \"\"\"\n",
    "    T, B, C = log_probs.shape\n",
    "    preds = log_probs.detach().cpu().argmax(dim=-1)  # (T, B)\n",
    "\n",
    "    decoded = []\n",
    "    for b in range(B):\n",
    "        T_b = input_lengths[b].item()\n",
    "        seq = preds[:T_b, b].tolist()\n",
    "\n",
    "        # CTC collapse: remove repeats & blanks\n",
    "        collapsed = []\n",
    "        prev = None\n",
    "        for s in seq:\n",
    "            if s == blank_id:\n",
    "                prev = None\n",
    "                continue\n",
    "            if s != prev:\n",
    "                collapsed.append(s)\n",
    "                prev = s\n",
    "        decoded.append(collapsed)\n",
    "    return decoded\n",
    "\n",
    "\n",
    "def phoneme_error_rate(pred_ids, target_ids):\n",
    "    \"\"\"\n",
    "    Classic edit distance / PER.\n",
    "    pred_ids, target_ids: list[int]\n",
    "    \"\"\"\n",
    "    m, n = len(target_ids), len(pred_ids)\n",
    "    dp = [[0]*(n+1) for _ in range(m+1)]\n",
    "    for i in range(m+1):\n",
    "        dp[i][0] = i\n",
    "    for j in range(n+1):\n",
    "        dp[0][j] = j\n",
    "\n",
    "    for i in range(1, m+1):\n",
    "        for j in range(1, n+1):\n",
    "            cost = 0 if target_ids[i-1] == pred_ids[j-1] else 1\n",
    "            dp[i][j] = min(\n",
    "                dp[i-1][j] + 1,      # delete\n",
    "                dp[i][j-1] + 1,      # insert\n",
    "                dp[i-1][j-1] + cost  # substitute\n",
    "            )\n",
    "    return dp[m][n] / max(1, m)\n",
    "\n",
    "\n",
    "def compute_batch_per(decoded_batch, flat_labels, label_lengths):\n",
    "    \"\"\"\n",
    "    decoded_batch: list[list[int]] length B\n",
    "    flat_labels: (sum_L,)\n",
    "    label_lengths: (B,)\n",
    "    Returns: average PER over batch\n",
    "    \"\"\"\n",
    "    idx = 0\n",
    "    per_sum = 0.0\n",
    "    B = len(decoded_batch)\n",
    "\n",
    "    for b in range(B):\n",
    "        L_b = label_lengths[b].item()\n",
    "        target_b = flat_labels[idx: idx + L_b].tolist()\n",
    "        idx += L_b\n",
    "\n",
    "        pred_b = decoded_batch[b]\n",
    "        per_sum += phoneme_error_rate(pred_b, target_b)\n",
    "\n",
    "    return per_sum / max(1, B)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# --- CORRECTED CONFIGURATION ---\n",
    "# We reserve 0 for CTC Blank. \n",
    "# Your original data has 41 classes (0-40).\n",
    "# We shift them to 1-41. So Total Classes = 42.\n",
    "BLANK_ID = 0\n",
    "NUM_CLASSES = 42  # 41 phonemes + 1 dedicated blank\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "class BrainCTCDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        df = df.copy()\n",
    "        # Filter rows without length\n",
    "        df = df[df[\"seq_len\"].notnull()]\n",
    "        self.df = df.reset_index(drop=True)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "\n",
    "        feats = row[\"neural_features\"]\n",
    "        T = int(row[\"n_time_steps\"])\n",
    "        L = int(row[\"seq_len\"])\n",
    "\n",
    "        # 1. Handle Features\n",
    "        if isinstance(feats, np.ndarray):\n",
    "            feats = torch.from_numpy(feats).float()\n",
    "        else:\n",
    "            feats = feats.float()\n",
    "\n",
    "        # 2. Handle Labels (THE CRITICAL FIX)\n",
    "        seq_ids = row[\"seq_class_ids\"]\n",
    "        # Take the valid sequence\n",
    "        raw_labels = torch.as_tensor(seq_ids[:L], dtype=torch.long)\n",
    "        \n",
    "        # SHIFT LABELS: 0 becomes 1, 40 becomes 41.\n",
    "        # Now 0 is free to be the CTC Blank.\n",
    "        labels = raw_labels + 1 \n",
    "\n",
    "        sample = {\n",
    "            \"feats\": feats,      \n",
    "            \"T\": T,\n",
    "            \"labels\": labels,    \n",
    "            \"L\": L,\n",
    "        }\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TRAINING CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "Starting Training...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 165\u001b[39m\n\u001b[32m    162\u001b[39m log_probs = F.log_softmax(logits, dim=\u001b[32m2\u001b[39m)\n\u001b[32m    164\u001b[39m loss = criterion(log_probs, labels, in_lens, lbl_lens)\n\u001b[32m--> \u001b[39m\u001b[32m165\u001b[39m \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    167\u001b[39m torch.nn.utils.clip_grad_norm_(model.parameters(), \u001b[32m1.0\u001b[39m)\n\u001b[32m    168\u001b[39m optimizer.step()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\btlim\\OneDrive\\Desktop\\brain_to_text_local\\.venv\\Lib\\site-packages\\torch\\_tensor.py:581\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    571\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    572\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    573\u001b[39m         Tensor.backward,\n\u001b[32m    574\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    579\u001b[39m         inputs=inputs,\n\u001b[32m    580\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m581\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    582\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    583\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\btlim\\OneDrive\\Desktop\\brain_to_text_local\\.venv\\Lib\\site-packages\\torch\\autograd\\__init__.py:347\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    342\u001b[39m     retain_graph = create_graph\n\u001b[32m    344\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    345\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    346\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m347\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    348\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    349\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    350\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    351\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    352\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    353\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    354\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\btlim\\OneDrive\\Desktop\\brain_to_text_local\\.venv\\Lib\\site-packages\\torch\\autograd\\graph.py:825\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    823\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    824\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m825\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    826\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    827\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    828\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    829\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# ==========================================\n",
    "# 1. CONFIGURATION & DATASET\n",
    "# ==========================================\n",
    "BATCH_SIZE = 64        # BiLSTM is light, we can use larger batch\n",
    "EPOCHS = 30\n",
    "LR_MAX = 2e-3          # Peak learning rate\n",
    "FEAT_DIM = 512\n",
    "BLANK_ID = 0\n",
    "NUM_CLASSES = 42       # 41 phonemes + 1 blank\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "print(f\"Device: {DEVICE}\")\n",
    "\n",
    "class BrainCTCDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        df = df.copy()\n",
    "        df = df[df[\"seq_len\"].notnull()]\n",
    "        self.df = df.reset_index(drop=True)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        feats = row[\"neural_features\"]\n",
    "        if isinstance(feats, np.ndarray):\n",
    "            feats = torch.from_numpy(feats).float()\n",
    "        else:\n",
    "            feats = feats.float()\n",
    "            \n",
    "        # LABELS: Shift +1 to avoid 0 collision\n",
    "        seq_ids = row[\"seq_class_ids\"]\n",
    "        L = int(row[\"seq_len\"])\n",
    "        raw_labels = torch.as_tensor(seq_ids[:L], dtype=torch.long)\n",
    "        labels = raw_labels + 1 \n",
    "\n",
    "        return {\n",
    "            \"feats\": feats,\n",
    "            \"T\": int(row[\"n_time_steps\"]),\n",
    "            \"labels\": labels,\n",
    "            \"L\": L\n",
    "        }\n",
    "\n",
    "# Re-create loaders with the fixed Dataset\n",
    "train_ds = BrainCTCDataset(train_df)\n",
    "val_ds   = BrainCTCDataset(val_df)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True,  collate_fn=ctc_collate_fn, num_workers=0)\n",
    "val_loader   = DataLoader(val_ds,   batch_size=BATCH_SIZE, shuffle=False, collate_fn=ctc_collate_fn, num_workers=0)\n",
    "\n",
    "# ==========================================\n",
    "# 2. THE ROBUST MODEL (Plan A)\n",
    "# ==========================================\n",
    "class RobustBiLSTM(nn.Module):\n",
    "    def __init__(self, feat_dim, num_classes, hidden=384, num_layers=4, dropout=0.3):\n",
    "        super().__init__()\n",
    "        self.ln = nn.LayerNorm(feat_dim)\n",
    "        self.projection = nn.Linear(feat_dim, hidden)\n",
    "        \n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=hidden,\n",
    "            hidden_size=hidden,\n",
    "            num_layers=num_layers,\n",
    "            dropout=dropout,\n",
    "            bidirectional=True,\n",
    "            batch_first=False\n",
    "        )\n",
    "        self.output_proj = nn.Linear(hidden * 2, num_classes)\n",
    "\n",
    "        # Initialize Bias to favor Blanks (Stability Hack)\n",
    "        init_bias = torch.zeros(num_classes)\n",
    "        init_bias[0] = 2.0 \n",
    "        self.output_proj.bias.data = init_bias\n",
    "\n",
    "    def forward(self, x, input_lengths):\n",
    "        # x: (Time, Batch, Feats)\n",
    "        x = self.ln(x)\n",
    "        x = self.projection(x)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        # Pack for efficiency\n",
    "        x_packed = nn.utils.rnn.pack_padded_sequence(x, input_lengths.cpu(), enforce_sorted=False)\n",
    "        out_packed, _ = self.lstm(x_packed)\n",
    "        out, _ = nn.utils.rnn.pad_packed_sequence(out_packed)\n",
    "        \n",
    "        logits = self.output_proj(out)\n",
    "        return logits\n",
    "\n",
    "# ==========================================\n",
    "# 3. UTILS (Decoding & PER)\n",
    "# ==========================================\n",
    "def greedy_decode(log_probs, input_lengths):\n",
    "    T, B, C = log_probs.shape\n",
    "    preds = log_probs.argmax(dim=-1).cpu()\n",
    "    decoded = []\n",
    "    for b in range(B):\n",
    "        length = input_lengths[b].item()\n",
    "        seq = preds[:length, b].tolist()\n",
    "        # Collapse CTC\n",
    "        collapsed = []\n",
    "        prev = None\n",
    "        for p in seq:\n",
    "            if p != BLANK_ID and p != prev:\n",
    "                collapsed.append(p)\n",
    "            prev = p\n",
    "        decoded.append(collapsed)\n",
    "    return decoded\n",
    "\n",
    "def compute_per(ref, hyp):\n",
    "    # Simple Levenshtein distance\n",
    "    import editdistance # Ensure this is installed, or use a custom function\n",
    "    # Fallback if library missing:\n",
    "    try:\n",
    "        d = editdistance.eval(ref, hyp)\n",
    "    except:\n",
    "        # Quick fallback implementation\n",
    "        m, n = len(ref), len(hyp)\n",
    "        dp = [[0]*(n+1) for _ in range(m+1)]\n",
    "        for i in range(m+1): dp[i][0] = i\n",
    "        for j in range(n+1): dp[0][j] = j\n",
    "        for i in range(1, m+1):\n",
    "            for j in range(1, n+1):\n",
    "                cost = 0 if ref[i-1] == hyp[j-1] else 1\n",
    "                dp[i][j] = min(dp[i-1][j]+1, dp[i][j-1]+1, dp[i-1][j-1]+cost)\n",
    "        d = dp[m][n]\n",
    "    return d / len(ref) if len(ref) > 0 else 0\n",
    "\n",
    "# ==========================================\n",
    "# 4. TRAINING LOOP\n",
    "# ==========================================\n",
    "model = RobustBiLSTM(FEAT_DIM, NUM_CLASSES).to(DEVICE)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LR_MAX, weight_decay=1e-2)\n",
    "scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "    optimizer, max_lr=LR_MAX, steps_per_epoch=len(train_loader), epochs=EPOCHS, pct_start=0.15\n",
    ")\n",
    "criterion = nn.CTCLoss(blank=BLANK_ID, zero_infinity=True)\n",
    "\n",
    "best_per = 1.0\n",
    "print(\"Starting Training...\")\n",
    "\n",
    "for epoch in range(1, EPOCHS+1):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    \n",
    "    for batch in train_loader:\n",
    "        feats, in_lens, labels, lbl_lens = batch\n",
    "        feats = feats.to(DEVICE).transpose(0, 1) # T, B, F\n",
    "        labels = labels.to(DEVICE)\n",
    "        lbl_lens = lbl_lens.to(DEVICE)\n",
    "        in_lens = in_lens.to(DEVICE)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        logits = model(feats, in_lens)\n",
    "        log_probs = F.log_softmax(logits, dim=2)\n",
    "        \n",
    "        loss = criterion(log_probs, labels, in_lens, lbl_lens)\n",
    "        loss.backward()\n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        train_loss += loss.item()\n",
    "        \n",
    "    avg_train_loss = train_loss / len(train_loader)\n",
    "    \n",
    "    # --- VALIDATION ---\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    val_per = 0\n",
    "    count = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            feats, in_lens, labels, lbl_lens = batch\n",
    "            feats = feats.to(DEVICE).transpose(0, 1)\n",
    "            labels = labels.to(DEVICE)\n",
    "            in_lens = in_lens.to(DEVICE)\n",
    "            lbl_lens = lbl_lens.to(DEVICE)\n",
    "            \n",
    "            logits = model(feats, in_lens)\n",
    "            log_probs = F.log_softmax(logits, dim=2)\n",
    "            val_loss += criterion(log_probs, labels, in_lens, lbl_lens).item()\n",
    "            \n",
    "            # Compute PER\n",
    "            decoded_batch = greedy_decode(log_probs, in_lens)\n",
    "            # Reconstruct targets from flat labels\n",
    "            idx = 0\n",
    "            for i, hyp in enumerate(decoded_batch):\n",
    "                L_target = lbl_lens[i].item()\n",
    "                ref = labels[idx:idx+L_target].cpu().tolist()\n",
    "                idx += L_target\n",
    "                val_per += compute_per(ref, hyp)\n",
    "                count += 1\n",
    "                \n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    avg_per = val_per / count\n",
    "    \n",
    "    print(f\"Epoch {epoch}: Train Loss={avg_train_loss:.4f} | Val Loss={avg_val_loss:.4f} | Val PER={avg_per:.4f}\")\n",
    "    \n",
    "    if avg_per < best_per:\n",
    "        best_per = avg_per\n",
    "        torch.save(model.state_dict(), \"best_bilstm_robust.pth\")\n",
    "        print(f\"  >>> New Best Model Saved! PER: {best_per:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# MISSING UTILITY FUNCTIONS\n",
    "# ==========================================\n",
    "\n",
    "def greedy_decode(log_probs, input_lengths):\n",
    "    \"\"\"\n",
    "    Decodes the CTC output by taking the most likely token at each step.\n",
    "    log_probs: (Time, Batch, Class)\n",
    "    input_lengths: (Batch,)\n",
    "    \"\"\"\n",
    "    # 1. Get max probability indices\n",
    "    preds = log_probs.argmax(dim=-1).cpu() \n",
    "    \n",
    "    decoded = []\n",
    "    for b in range(log_probs.shape[1]): # Iterate over batch\n",
    "        length = input_lengths[b].item()\n",
    "        seq = preds[:length, b].tolist()\n",
    "        \n",
    "        # 2. CTC Collapse: Remove duplicates & Blanks\n",
    "        collapsed = []\n",
    "        prev = None\n",
    "        for p in seq:\n",
    "            if p != 0 and p != prev: # 0 is BLANK_ID\n",
    "                collapsed.append(p)\n",
    "            prev = p\n",
    "        decoded.append(collapsed)\n",
    "    return decoded\n",
    "\n",
    "def compute_per(ref, hyp):\n",
    "    \"\"\"\n",
    "    Calculates Phoneme Error Rate (Levenshtein Distance / Reference Length).\n",
    "    \"\"\"\n",
    "    # Simple DP implementation of Edit Distance\n",
    "    m, n = len(ref), len(hyp)\n",
    "    \n",
    "    # Create a table to store results of subproblems\n",
    "    dp = [[0] * (n + 1) for _ in range(m + 1)]\n",
    " \n",
    "    # Fill dp[][] in bottom up manner\n",
    "    for i in range(m + 1):\n",
    "        for j in range(n + 1):\n",
    "            if i == 0:\n",
    "                dp[i][j] = j    # Min. operations = j\n",
    "            elif j == 0:\n",
    "                dp[i][j] = i    # Min. operations = i\n",
    "            elif ref[i - 1] == hyp[j - 1]:\n",
    "                dp[i][j] = dp[i - 1][j - 1]\n",
    "            else:\n",
    "                dp[i][j] = 1 + min(dp[i][j - 1],      # Insert\n",
    "                                   dp[i - 1][j],      # Remove\n",
    "                                   dp[i - 1][j - 1])  # Replace\n",
    " \n",
    "    distance = dp[m][n]\n",
    "    \n",
    "    if len(ref) == 0:\n",
    "        return 0.0 if len(hyp) == 0 else 1.0\n",
    "        \n",
    "    return distance / len(ref)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TRAINING CODE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "Starting Conformer Training...\n",
      "Epoch 1: Loss=3.8296 | Val PER=0.9627\n",
      "  >>> New SOTA Model Saved! PER: 0.9627\n",
      "Epoch 2: Loss=2.3755 | Val PER=0.7442\n",
      "  >>> New SOTA Model Saved! PER: 0.7442\n",
      "Epoch 3: Loss=1.5732 | Val PER=0.4319\n",
      "  >>> New SOTA Model Saved! PER: 0.4319\n",
      "Epoch 4: Loss=1.1913 | Val PER=0.3752\n",
      "  >>> New SOTA Model Saved! PER: 0.3752\n",
      "Epoch 5: Loss=1.0470 | Val PER=0.3726\n",
      "  >>> New SOTA Model Saved! PER: 0.3726\n",
      "Epoch 6: Loss=0.9905 | Val PER=0.3602\n",
      "  >>> New SOTA Model Saved! PER: 0.3602\n",
      "Epoch 7: Loss=1.0354 | Val PER=0.4226\n",
      "Epoch 8: Loss=1.0560 | Val PER=0.3957\n",
      "Epoch 9: Loss=1.0124 | Val PER=0.3959\n",
      "Epoch 10: Loss=0.9706 | Val PER=0.3629\n",
      "Epoch 11: Loss=0.9913 | Val PER=0.3631\n",
      "Epoch 12: Loss=0.9638 | Val PER=0.3806\n",
      "Epoch 13: Loss=0.9231 | Val PER=0.3856\n",
      "Epoch 14: Loss=1.0044 | Val PER=0.3847\n",
      "Epoch 15: Loss=1.0976 | Val PER=0.4009\n",
      "Epoch 16: Loss=0.9424 | Val PER=0.3253\n",
      "  >>> New SOTA Model Saved! PER: 0.3253\n",
      "Epoch 17: Loss=0.8646 | Val PER=0.3245\n",
      "  >>> New SOTA Model Saved! PER: 0.3245\n",
      "Epoch 18: Loss=0.7876 | Val PER=0.2946\n",
      "  >>> New SOTA Model Saved! PER: 0.2946\n",
      "Epoch 19: Loss=0.7415 | Val PER=0.2851\n",
      "  >>> New SOTA Model Saved! PER: 0.2851\n",
      "Epoch 20: Loss=0.6754 | Val PER=0.2655\n",
      "  >>> New SOTA Model Saved! PER: 0.2655\n",
      "Epoch 21: Loss=0.6086 | Val PER=0.2629\n",
      "  >>> New SOTA Model Saved! PER: 0.2629\n",
      "Epoch 22: Loss=0.6145 | Val PER=0.2712\n",
      "Epoch 23: Loss=0.5874 | Val PER=0.2402\n",
      "  >>> New SOTA Model Saved! PER: 0.2402\n",
      "Epoch 24: Loss=0.5478 | Val PER=0.2271\n",
      "  >>> New SOTA Model Saved! PER: 0.2271\n",
      "Epoch 25: Loss=0.5194 | Val PER=0.2211\n",
      "  >>> New SOTA Model Saved! PER: 0.2211\n",
      "Epoch 26: Loss=0.4932 | Val PER=0.2115\n",
      "  >>> New SOTA Model Saved! PER: 0.2115\n",
      "Epoch 27: Loss=0.4645 | Val PER=0.2183\n",
      "Epoch 28: Loss=0.4216 | Val PER=0.2059\n",
      "  >>> New SOTA Model Saved! PER: 0.2059\n",
      "Epoch 29: Loss=0.3930 | Val PER=0.2016\n",
      "  >>> New SOTA Model Saved! PER: 0.2016\n",
      "Epoch 30: Loss=0.3711 | Val PER=0.1939\n",
      "  >>> New SOTA Model Saved! PER: 0.1939\n",
      "Epoch 31: Loss=0.3430 | Val PER=0.1864\n",
      "  >>> New SOTA Model Saved! PER: 0.1864\n",
      "Epoch 32: Loss=0.3238 | Val PER=0.1828\n",
      "  >>> New SOTA Model Saved! PER: 0.1828\n",
      "Epoch 33: Loss=0.3044 | Val PER=0.1800\n",
      "  >>> New SOTA Model Saved! PER: 0.1800\n",
      "Epoch 34: Loss=0.2893 | Val PER=0.1788\n",
      "  >>> New SOTA Model Saved! PER: 0.1788\n",
      "Epoch 35: Loss=0.2757 | Val PER=0.1757\n",
      "  >>> New SOTA Model Saved! PER: 0.1757\n",
      "Epoch 36: Loss=0.2689 | Val PER=0.1753\n",
      "  >>> New SOTA Model Saved! PER: 0.1753\n",
      "Epoch 37: Loss=0.2592 | Val PER=0.1745\n",
      "  >>> New SOTA Model Saved! PER: 0.1745\n",
      "Epoch 38: Loss=0.2534 | Val PER=0.1749\n",
      "Epoch 39: Loss=0.2513 | Val PER=0.1743\n",
      "  >>> New SOTA Model Saved! PER: 0.1743\n",
      "Epoch 40: Loss=0.2508 | Val PER=0.1741\n",
      "  >>> New SOTA Model Saved! PER: 0.1741\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchaudio\n",
    "\n",
    "# ==========================================\n",
    "# 1. CONFIGURATION\n",
    "# ==========================================\n",
    "BATCH_SIZE = 16          # Conformers are heavy; reduce batch size\n",
    "EPOCHS = 40              # They need longer to converge\n",
    "LR_MAX = 5e-4            # Lower LR is safer for Transformers\n",
    "FEAT_DIM = 512\n",
    "NUM_CLASSES = 42         # 41 phonemes + 1 blank (Fixed!)\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "print(f\"Device: {DEVICE}\")\n",
    "\n",
    "# ==========================================\n",
    "# 2. THE SOTA MODEL (Conformer)\n",
    "# ==========================================\n",
    "class ConformerCTC(nn.Module):\n",
    "    def __init__(self, feat_dim, num_classes, d_model=256, n_head=4, num_layers=6):\n",
    "        super().__init__()\n",
    "        \n",
    "        # 1. Adapter: Compress 512 -> 256\n",
    "        self.projection = nn.Linear(feat_dim, d_model)\n",
    "        \n",
    "        # 2. The Brain: Conformer Encoder\n",
    "        self.conformer = torchaudio.models.Conformer(\n",
    "            input_dim=d_model,\n",
    "            num_heads=n_head,\n",
    "            ffn_dim=d_model * 4,\n",
    "            num_layers=num_layers,\n",
    "            depthwise_conv_kernel_size=31,\n",
    "            dropout=0.1\n",
    "        )\n",
    "        \n",
    "        # 3. Output Head\n",
    "        self.output_proj = nn.Linear(d_model, num_classes)\n",
    "        \n",
    "        # Bias Init Trick (Stability)\n",
    "        init_bias = torch.zeros(num_classes)\n",
    "        init_bias[0] = 2.0 \n",
    "        self.output_proj.bias.data = init_bias\n",
    "\n",
    "    def forward(self, x, input_lengths):\n",
    "        # Input x: (Time, Batch, Feats) -> Standard for our loader\n",
    "        \n",
    "        # A. Project & Permute\n",
    "        x = self.projection(x)  # (T, B, 256)\n",
    "        x = x.transpose(0, 1)   # (B, T, 256) -> Conformer expects Batch First\n",
    "        \n",
    "        # B. Conformer Pass\n",
    "        # Returns: output, lengths\n",
    "        out, _ = self.conformer(x, input_lengths)\n",
    "        \n",
    "        # C. Permute Back\n",
    "        out = out.transpose(0, 1) # (T, B, 256)\n",
    "        \n",
    "        # D. Project to Classes\n",
    "        logits = self.output_proj(out)\n",
    "        return logits\n",
    "\n",
    "# ==========================================\n",
    "# 3. DATA AUGMENTATION (SpecAugment)\n",
    "# ==========================================\n",
    "# Crucial for Conformers to avoid overfitting\n",
    "class SpecAugment(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.freq_mask = torchaudio.transforms.FrequencyMasking(freq_mask_param=20)\n",
    "        self.time_mask = torchaudio.transforms.TimeMasking(time_mask_param=50)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (Time, Batch, Feats) -> Permute to (Batch, Feats, Time) for Torchaudio\n",
    "        x = x.permute(1, 2, 0)\n",
    "        x = self.freq_mask(x)\n",
    "        x = self.time_mask(x)\n",
    "        x = x.permute(2, 0, 1) # Back to (Time, Batch, Feats)\n",
    "        return x\n",
    "\n",
    "# ==========================================\n",
    "# 4. TRAINING LOOP (With Schedulers)\n",
    "# ==========================================\n",
    "model = ConformerCTC(FEAT_DIM, NUM_CLASSES).to(DEVICE)\n",
    "augmenter = SpecAugment().to(DEVICE)\n",
    "\n",
    "# Conformer Needs AdamW + Cosine Scheduler\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LR_MAX, weight_decay=1e-2)\n",
    "scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "    optimizer, max_lr=LR_MAX, steps_per_epoch=len(train_loader), epochs=EPOCHS, pct_start=0.15\n",
    ")\n",
    "criterion = nn.CTCLoss(blank=0, zero_infinity=True)\n",
    "\n",
    "best_per = 1.0\n",
    "print(\"Starting Conformer Training...\")\n",
    "\n",
    "for epoch in range(1, EPOCHS+1):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    \n",
    "    for batch in train_loader:\n",
    "        feats, in_lens, labels, lbl_lens = batch\n",
    "        feats = feats.to(DEVICE).transpose(0, 1)\n",
    "        labels = labels.to(DEVICE)\n",
    "        lbl_lens = lbl_lens.to(DEVICE)\n",
    "        in_lens = in_lens.to(DEVICE)\n",
    "        \n",
    "        # Apply Augmentation\n",
    "        feats = augmenter(feats)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        logits = model(feats, in_lens)\n",
    "        log_probs = F.log_softmax(logits, dim=2)\n",
    "        \n",
    "        loss = criterion(log_probs, labels, in_lens, lbl_lens)\n",
    "        loss.backward()\n",
    "        \n",
    "        # Clip Gradients (Essential for Conformer)\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        train_loss += loss.item()\n",
    "        \n",
    "    avg_train_loss = train_loss / len(train_loader)\n",
    "    \n",
    "    # --- VALIDATION (No Augment) ---\n",
    "    model.eval()\n",
    "    val_per = 0\n",
    "    count = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            feats, in_lens, labels, lbl_lens = batch\n",
    "            feats = feats.to(DEVICE).transpose(0, 1)\n",
    "            labels = labels.to(DEVICE)\n",
    "            in_lens = in_lens.to(DEVICE)\n",
    "            lbl_lens = lbl_lens.to(DEVICE)\n",
    "            \n",
    "            logits = model(feats, in_lens)\n",
    "            log_probs = F.log_softmax(logits, dim=2)\n",
    "            \n",
    "            # Simple Greedy Decode for monitoring\n",
    "            decoded_batch = greedy_decode(log_probs, in_lens)\n",
    "            idx = 0\n",
    "            for i, hyp in enumerate(decoded_batch):\n",
    "                L_target = lbl_lens[i].item()\n",
    "                ref = labels[idx:idx+L_target].cpu().tolist()\n",
    "                idx += L_target\n",
    "                val_per += compute_per(ref, hyp)\n",
    "                count += 1\n",
    "                \n",
    "    avg_per = val_per / count\n",
    "    print(f\"Epoch {epoch}: Loss={avg_train_loss:.4f} | Val PER={avg_per:.4f}\")\n",
    "    \n",
    "    if avg_per < best_per:\n",
    "        best_per = avg_per\n",
    "        torch.save(model.state_dict(), \"best_conformer_sota.pth\")\n",
    "        print(f\"  >>> New SOTA Model Saved! PER: {best_per:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchaudio\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from pyctcdecode import build_ctcdecoder\n",
    "\n",
    "# ==========================================\n",
    "# 1. CONFIGURATION\n",
    "# ==========================================\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "MODEL_PATH = \"best_conformer_sota.pth\"\n",
    "OUTPUT_FILE = \"submission_english_fixed.csv\"\n",
    "FEAT_DIM = 512\n",
    "NUM_CLASSES = 42\n",
    "\n",
    "LOGIT_TO_PHONEME = [\n",
    "    'BLANK', 'AA', 'AE', 'AH', 'AO', 'AW', 'AY', 'B', 'CH', 'D', 'DH', \n",
    "    'EH', 'ER', 'EY', 'F', 'G', 'HH', 'IH', 'IY', 'JH', 'K', 'L', 'M', \n",
    "    'N', 'NG', 'OW', 'OY', 'P', 'R', 'S', 'SH', 'T', 'TH', 'UH', 'UW', \n",
    "    'V', 'W', 'Y', 'Z', 'ZH', ' | '\n",
    "]\n",
    "\n",
    "# ==========================================\n",
    "# 2. DICTIONARY & FIXING LOGIC\n",
    "# ==========================================\n",
    "print(\"Initializing Dictionary...\")\n",
    "try: nltk.data.find('corpora/cmudict')\n",
    "except LookupError: nltk.download('cmudict', quiet=True)\n",
    "from nltk.corpus import cmudict\n",
    "d = cmudict.dict()\n",
    "\n",
    "# 1. Build Phoneme Map\n",
    "VALID_PHONEMES = set(LOGIT_TO_PHONEME[1:]) \n",
    "phoneme_to_word = {}\n",
    "for word, prons in d.items():\n",
    "    for pron in prons:\n",
    "        clean_pron = tuple([p.strip('012') for p in pron])\n",
    "        if clean_pron not in phoneme_to_word:\n",
    "            phoneme_to_word[clean_pron] = word\n",
    "        elif len(word) < len(phoneme_to_word[clean_pron]):\n",
    "            phoneme_to_word[clean_pron] = word\n",
    "\n",
    "def split_phoneme_string(smushed_str):\n",
    "    tokens = []\n",
    "    i = 0\n",
    "    n = len(smushed_str)\n",
    "    while i < n:\n",
    "        match_found = False\n",
    "        for length in [3, 2, 1]: \n",
    "            if i + length <= n:\n",
    "                chunk = smushed_str[i : i+length]\n",
    "                if chunk in VALID_PHONEMES:\n",
    "                    tokens.append(chunk)\n",
    "                    i += length\n",
    "                    match_found = True\n",
    "                    break\n",
    "        if not match_found: i += 1\n",
    "    return tuple(tokens)\n",
    "\n",
    "# 2. Manual Fixes (Crucial for 0.48 score)\n",
    "COMMON_FIXES = {\n",
    "    \"AY\": \"I\", \"EY\": \"A\", \"WIHDH\": \"WITH\", \"DHAH\": \"THE\", \"AHND\": \"AND\",\n",
    "    \"TAYERD\": \"TIRED\", \"GEHNT\": \"GET\", \"SAHNG\": \"SONG\", \"DEHS\": \"THIS\",\n",
    "    \"HHERR\": \"HER\", \"YUWR\": \"YOUR\", \"THIHNGK\": \"THINK\", \"HHAED\": \"HAD\",\n",
    "    \"DHEY\": \"THEY\", \"HHAEV\": \"HAVE\", \"EHNJHOY\": \"ENJOY\", \"MUWVD\": \"MOVED\",\n",
    "    \"SIHTIY\": \"CITY\", \"WERK\": \"WORK\", \"SIYZ\": \"SEES\", \"DHEHRZ\": \"THERES\"\n",
    "}\n",
    "\n",
    "def translate_and_fix(phoneme_text):\n",
    "    if not phoneme_text: return \"\"\n",
    "    raw_groups = phoneme_text.split('|')\n",
    "    sentence = []\n",
    "    for group in raw_groups:\n",
    "        clean_str = group.strip()\n",
    "        if not clean_str: continue\n",
    "        \n",
    "        # A. Try Dictionary\n",
    "        ph_tuple = split_phoneme_string(clean_str)\n",
    "        if ph_tuple in phoneme_to_word:\n",
    "            word = phoneme_to_word[ph_tuple]\n",
    "        else:\n",
    "            word = clean_str\n",
    "            \n",
    "        # B. Apply Fixes\n",
    "        if word in COMMON_FIXES:\n",
    "            word = COMMON_FIXES[word]\n",
    "            \n",
    "        sentence.append(word)\n",
    "    return \" \".join(sentence)\n",
    "\n",
    "# ==========================================\n",
    "# 3. MODEL (Conformer SOTA)\n",
    "# ==========================================\n",
    "class ConformerCTC(nn.Module):\n",
    "    def __init__(self, feat_dim, num_classes, d_model=256, n_head=4, num_layers=6):\n",
    "        super().__init__()\n",
    "        self.projection = nn.Linear(feat_dim, d_model)\n",
    "        self.conformer = torchaudio.models.Conformer(\n",
    "            input_dim=d_model, num_heads=n_head, ffn_dim=d_model * 4,\n",
    "            num_layers=num_layers, depthwise_conv_kernel_size=31, dropout=0.1\n",
    "        )\n",
    "        self.output_proj = nn.Linear(d_model, num_classes)\n",
    "\n",
    "    def forward(self, x, input_lengths):\n",
    "        x = self.projection(x)\n",
    "        x = x.transpose(0, 1) \n",
    "        out, _ = self.conformer(x, input_lengths)\n",
    "        out = out.transpose(0, 1) \n",
    "        return self.output_proj(out)\n",
    "\n",
    "# ==========================================\n",
    "# 4. INFERENCE\n",
    "# ==========================================\n",
    "class TestDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "    def __len__(self): return len(self.df)\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        feats = row[\"neural_features\"]\n",
    "        if isinstance(feats, np.ndarray): feats = torch.from_numpy(feats).float()\n",
    "        else: feats = feats.float()\n",
    "        return {\"feats\": feats, \"T\": int(row[\"n_time_steps\"])}\n",
    "\n",
    "print(\"Loading SOTA Model...\")\n",
    "model = ConformerCTC(FEAT_DIM, NUM_CLASSES).to(DEVICE)\n",
    "model.load_state_dict(torch.load(MODEL_PATH, map_location=DEVICE))\n",
    "model.eval()\n",
    "\n",
    "# Decoder (Windows Safe: Alpha=0)\n",
    "vocab_list = [x for x in LOGIT_TO_PHONEME if x != 'BLANK'] + ['BLANK']\n",
    "decoder = build_ctcdecoder(labels=vocab_list, kenlm_model_path=None, alpha=0.0, beta=1.0)\n",
    "\n",
    "test_ds = TestDataset(test_df)\n",
    "test_loader = DataLoader(test_ds, batch_size=1, shuffle=False)\n",
    "\n",
    "final_predictions = []\n",
    "print(f\"Generating Predictions...\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, batch in enumerate(test_loader):\n",
    "        feats = batch['feats'].to(DEVICE).transpose(0, 1)\n",
    "        in_lens = torch.tensor([batch['T']], device=DEVICE)\n",
    "        \n",
    "        # 1. Inference\n",
    "        logits = model(feats, in_lens)\n",
    "        probs = F.softmax(logits, dim=2)\n",
    "        probs_np = probs.transpose(0, 1).cpu().numpy()[0]\n",
    "        \n",
    "        # 2. Decode\n",
    "        probs_adjusted = np.roll(probs_np, -1, axis=-1)\n",
    "        phoneme_text = decoder.decode(probs_adjusted)\n",
    "        \n",
    "        # 3. Translate & Fix\n",
    "        english_text = translate_and_fix(phoneme_text)\n",
    "        \n",
    "        final_predictions.append(english_text)\n",
    "        \n",
    "        if i % 100 == 0: print(f\"Sample {i}: {english_text}\")\n",
    "\n",
    "# Save\n",
    "submission = pd.DataFrame({\"id\": range(len(final_predictions)), \"text\": final_predictions})\n",
    "submission.to_csv(OUTPUT_FILE, index=False)\n",
    "print(f\"✅ Saved {OUTPUT_FILE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TRAINING CODE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "Starting Transformer Training...\n",
      "Epoch 1: Loss=3.6254 | Val PER=1.0000\n",
      "Epoch 2: Loss=2.7583 | Val PER=1.0000\n",
      "  >>> New Best Transformer! PER: 1.0000\n",
      "Epoch 3: Loss=2.4018 | Val PER=0.9999\n",
      "  >>> New Best Transformer! PER: 0.9999\n",
      "Epoch 4: Loss=2.0975 | Val PER=0.9999\n",
      "  >>> New Best Transformer! PER: 0.9999\n",
      "Epoch 5: Loss=1.8835 | Val PER=0.8084\n",
      "  >>> New Best Transformer! PER: 0.8084\n",
      "Epoch 6: Loss=1.7686 | Val PER=0.8264\n",
      "Epoch 7: Loss=1.8621 | Val PER=0.8761\n",
      "Epoch 8: Loss=1.9263 | Val PER=0.8525\n",
      "Epoch 9: Loss=1.9562 | Val PER=0.8611\n",
      "Epoch 10: Loss=1.9670 | Val PER=0.9834\n",
      "Epoch 11: Loss=1.9892 | Val PER=0.8883\n",
      "Epoch 12: Loss=1.9398 | Val PER=0.9051\n",
      "Epoch 13: Loss=1.9295 | Val PER=0.9044\n",
      "Epoch 14: Loss=1.9167 | Val PER=0.8907\n",
      "Epoch 15: Loss=1.8776 | Val PER=0.8587\n",
      "Epoch 16: Loss=1.8850 | Val PER=0.8628\n",
      "Epoch 17: Loss=1.8346 | Val PER=0.8628\n",
      "Epoch 18: Loss=1.8048 | Val PER=0.8768\n",
      "Epoch 19: Loss=1.7834 | Val PER=0.8710\n",
      "Epoch 20: Loss=1.7110 | Val PER=0.8309\n",
      "Epoch 21: Loss=1.7101 | Val PER=0.8123\n",
      "Epoch 22: Loss=1.6765 | Val PER=0.8202\n",
      "Epoch 23: Loss=1.6251 | Val PER=0.7996\n",
      "  >>> New Best Transformer! PER: 0.7996\n",
      "Epoch 24: Loss=1.5896 | Val PER=0.7979\n",
      "  >>> New Best Transformer! PER: 0.7979\n",
      "Epoch 25: Loss=1.5359 | Val PER=0.7419\n",
      "  >>> New Best Transformer! PER: 0.7419\n",
      "Epoch 26: Loss=1.4868 | Val PER=0.7408\n",
      "  >>> New Best Transformer! PER: 0.7408\n",
      "Epoch 27: Loss=1.4481 | Val PER=0.6948\n",
      "  >>> New Best Transformer! PER: 0.6948\n",
      "Epoch 28: Loss=1.4044 | Val PER=0.6675\n",
      "  >>> New Best Transformer! PER: 0.6675\n",
      "Epoch 29: Loss=1.3483 | Val PER=0.6390\n",
      "  >>> New Best Transformer! PER: 0.6390\n",
      "Epoch 30: Loss=1.3039 | Val PER=0.6454\n",
      "Epoch 31: Loss=1.2467 | Val PER=0.6144\n",
      "  >>> New Best Transformer! PER: 0.6144\n",
      "Epoch 32: Loss=1.1987 | Val PER=0.6123\n",
      "  >>> New Best Transformer! PER: 0.6123\n",
      "Epoch 33: Loss=1.1634 | Val PER=0.5944\n",
      "  >>> New Best Transformer! PER: 0.5944\n",
      "Epoch 34: Loss=1.1280 | Val PER=0.5867\n",
      "  >>> New Best Transformer! PER: 0.5867\n",
      "Epoch 35: Loss=1.0989 | Val PER=0.5639\n",
      "  >>> New Best Transformer! PER: 0.5639\n",
      "Epoch 36: Loss=1.0718 | Val PER=0.5706\n",
      "Epoch 37: Loss=1.0530 | Val PER=0.5563\n",
      "  >>> New Best Transformer! PER: 0.5563\n",
      "Epoch 38: Loss=1.0494 | Val PER=0.5583\n",
      "Epoch 39: Loss=1.0366 | Val PER=0.5572\n",
      "Epoch 40: Loss=1.0343 | Val PER=0.5576\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import torchaudio\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "# ==========================================\n",
    "# 1. CONFIGURATION\n",
    "# ==========================================\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 40\n",
    "LR_MAX = 1e-3\n",
    "FEAT_DIM = 512\n",
    "NUM_CLASSES = 42\n",
    "\n",
    "print(f\"Device: {DEVICE}\")\n",
    "\n",
    "# ==========================================\n",
    "# 2. MISSING CLASS: SpecAugment\n",
    "# ==========================================\n",
    "class SpecAugment(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.freq_mask = torchaudio.transforms.FrequencyMasking(freq_mask_param=20)\n",
    "        self.time_mask = torchaudio.transforms.TimeMasking(time_mask_param=50)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Input: (Time, Batch, Feats)\n",
    "        # Permute for Torchaudio: (Batch, Feats, Time)\n",
    "        x = x.permute(1, 2, 0)\n",
    "        x = self.freq_mask(x)\n",
    "        x = self.time_mask(x)\n",
    "        # Back to: (Time, Batch, Feats)\n",
    "        return x.permute(2, 0, 1)\n",
    "\n",
    "# ==========================================\n",
    "# 3. MODEL COMPONENTS\n",
    "# ==========================================\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "        pe = torch.zeros(max_len, 1, d_model)\n",
    "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0)]\n",
    "        return self.dropout(x)\n",
    "\n",
    "class TransformerCTC(nn.Module):\n",
    "    def __init__(self, feat_dim, num_classes, d_model=256, nhead=4, num_layers=6):\n",
    "        super().__init__()\n",
    "        self.input_proj = nn.Linear(feat_dim, d_model)\n",
    "        self.pos_encoder = PositionalEncoding(d_model)\n",
    "        encoder_layers = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model, nhead=nhead, dim_feedforward=d_model*4, dropout=0.1\n",
    "        )\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layers, num_layers)\n",
    "        self.output_proj = nn.Linear(d_model, num_classes)\n",
    "        \n",
    "        # Stability Hack\n",
    "        init_bias = torch.zeros(num_classes)\n",
    "        init_bias[0] = 2.0 \n",
    "        self.output_proj.bias.data = init_bias\n",
    "\n",
    "    def forward(self, x, input_lengths):\n",
    "        x = self.input_proj(x)\n",
    "        x = self.pos_encoder(x)\n",
    "        out = self.transformer_encoder(x)\n",
    "        return self.output_proj(out)\n",
    "\n",
    "# ==========================================\n",
    "# 4. TRAINING SETUP\n",
    "# ==========================================\n",
    "# Ensure DataLoaders exist. If 'train_loader' is missing, \n",
    "# re-run the cell that defines BrainCTCDataset and train_loader!\n",
    "\n",
    "model = TransformerCTC(FEAT_DIM, NUM_CLASSES).to(DEVICE)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LR_MAX, weight_decay=1e-2)\n",
    "criterion = nn.CTCLoss(blank=0, zero_infinity=True)\n",
    "\n",
    "# Now this will work because SpecAugment is defined above\n",
    "augmenter = SpecAugment().to(DEVICE)\n",
    "\n",
    "# Scheduler\n",
    "try:\n",
    "    scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "        optimizer, max_lr=LR_MAX, steps_per_epoch=len(train_loader), epochs=EPOCHS, pct_start=0.2\n",
    "    )\n",
    "except NameError:\n",
    "    print(\"❌ Error: 'train_loader' is missing. Please re-run the cell defining your DataLoaders.\")\n",
    "    raise\n",
    "\n",
    "# Utils (if missing)\n",
    "def greedy_decode(log_probs, input_lengths):\n",
    "    preds = log_probs.argmax(dim=-1).cpu()\n",
    "    decoded = []\n",
    "    for b in range(log_probs.shape[1]):\n",
    "        L = input_lengths[b].item()\n",
    "        seq = preds[:L, b].tolist()\n",
    "        flat = []\n",
    "        prev = None\n",
    "        for p in seq:\n",
    "            if p != 0 and p != prev:\n",
    "                flat.append(p)\n",
    "            prev = p\n",
    "        decoded.append(flat)\n",
    "    return decoded\n",
    "\n",
    "def compute_per(ref, hyp):\n",
    "    import editdistance\n",
    "    d = editdistance.eval(ref, hyp)\n",
    "    return d / len(ref) if len(ref) > 0 else 0\n",
    "\n",
    "# ==========================================\n",
    "# 5. TRAINING LOOP\n",
    "# ==========================================\n",
    "print(\"Starting Transformer Training...\")\n",
    "best_per = 1.0\n",
    "\n",
    "for epoch in range(1, EPOCHS+1):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    \n",
    "    for batch in train_loader:\n",
    "        feats, in_lens, labels, lbl_lens = batch\n",
    "        feats = feats.to(DEVICE).transpose(0, 1)\n",
    "        labels = labels.to(DEVICE)\n",
    "        lbl_lens = lbl_lens.to(DEVICE)\n",
    "        in_lens = in_lens.to(DEVICE)\n",
    "        \n",
    "        # Augment\n",
    "        feats = augmenter(feats)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        logits = model(feats, in_lens)\n",
    "        log_probs = F.log_softmax(logits, dim=2)\n",
    "        \n",
    "        loss = criterion(log_probs, labels, in_lens, lbl_lens)\n",
    "        loss.backward()\n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        train_loss += loss.item()\n",
    "        \n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_per = 0\n",
    "    count = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            feats, in_lens, labels, lbl_lens = batch\n",
    "            feats = feats.to(DEVICE).transpose(0, 1)\n",
    "            labels = labels.to(DEVICE)\n",
    "            in_lens = in_lens.to(DEVICE)\n",
    "            lbl_lens = lbl_lens.to(DEVICE)\n",
    "            \n",
    "            logits = model(feats, in_lens)\n",
    "            decoded = greedy_decode(F.log_softmax(logits, dim=2), in_lens)\n",
    "            \n",
    "            idx = 0\n",
    "            for i, hyp in enumerate(decoded):\n",
    "                L_target = lbl_lens[i].item()\n",
    "                ref = labels[idx:idx+L_target].cpu().tolist()\n",
    "                idx += L_target\n",
    "                val_per += compute_per(ref, hyp)\n",
    "                count += 1\n",
    "                \n",
    "    avg_per = val_per / count\n",
    "    print(f\"Epoch {epoch}: Loss={train_loss/len(train_loader):.4f} | Val PER={avg_per:.4f}\")\n",
    "    \n",
    "    if avg_per < best_per:\n",
    "        best_per = avg_per\n",
    "        torch.save(model.state_dict(), \"best_transformer.pth\")\n",
    "        print(f\"  >>> New Best Transformer! PER: {best_per:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchaudio\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "# ==========================================\n",
    "# 1. CONFIG (The Tweak)\n",
    "# ==========================================\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS = 40\n",
    "LR_MAX = 5e-4\n",
    "FEAT_DIM = 512\n",
    "NUM_CLASSES = 42\n",
    "\n",
    "# CHANGE: Deeper model (8 layers instead of 6) + More Dropout\n",
    "NUM_LAYERS = 8 \n",
    "DROPOUT = 0.15 \n",
    "\n",
    "print(f\"Device: {DEVICE}\")\n",
    "print(f\"Training Conformer V2 (Layers={NUM_LAYERS}, Dropout={DROPOUT})...\")\n",
    "\n",
    "# ==========================================\n",
    "# 2. MODEL DEFINITION\n",
    "# ==========================================\n",
    "class ConformerCTC(nn.Module):\n",
    "    def __init__(self, feat_dim, num_classes, d_model=256, n_head=4, num_layers=6, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.projection = nn.Linear(feat_dim, d_model)\n",
    "        self.conformer = torchaudio.models.Conformer(\n",
    "            input_dim=d_model,\n",
    "            num_heads=n_head,\n",
    "            ffn_dim=d_model * 4,\n",
    "            num_layers=num_layers,\n",
    "            depthwise_conv_kernel_size=31,\n",
    "            dropout=dropout # Increased dropout makes it different!\n",
    "        )\n",
    "        self.output_proj = nn.Linear(d_model, num_classes)\n",
    "        # Bias Hack\n",
    "        init_bias = torch.zeros(num_classes)\n",
    "        init_bias[0] = 2.0 \n",
    "        self.output_proj.bias.data = init_bias\n",
    "\n",
    "    def forward(self, x, input_lengths):\n",
    "        x = self.projection(x)\n",
    "        x = x.transpose(0, 1) \n",
    "        out, _ = self.conformer(x, input_lengths)\n",
    "        out = out.transpose(0, 1) \n",
    "        return self.output_proj(out)\n",
    "\n",
    "class SpecAugment(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.freq_mask = torchaudio.transforms.FrequencyMasking(freq_mask_param=20)\n",
    "        self.time_mask = torchaudio.transforms.TimeMasking(time_mask_param=50)\n",
    "    def forward(self, x):\n",
    "        x = x.permute(1, 2, 0)\n",
    "        x = self.freq_mask(x)\n",
    "        x = self.time_mask(x)\n",
    "        return x.permute(2, 0, 1)\n",
    "\n",
    "# ==========================================\n",
    "# 3. SETUP & TRAINING\n",
    "# ==========================================\n",
    "# Re-using loaders from memory (train_loader, val_loader)\n",
    "model = ConformerCTC(FEAT_DIM, NUM_CLASSES, num_layers=NUM_LAYERS, dropout=DROPOUT).to(DEVICE)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LR_MAX, weight_decay=1e-2)\n",
    "criterion = nn.CTCLoss(blank=0, zero_infinity=True)\n",
    "scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "    optimizer, max_lr=LR_MAX, steps_per_epoch=len(train_loader), epochs=EPOCHS, pct_start=0.15\n",
    ")\n",
    "augmenter = SpecAugment().to(DEVICE)\n",
    "\n",
    "# Utils (Standard)\n",
    "def greedy_decode(log_probs, input_lengths):\n",
    "    preds = log_probs.argmax(dim=-1).cpu()\n",
    "    decoded = []\n",
    "    for b in range(log_probs.shape[1]):\n",
    "        L = input_lengths[b].item()\n",
    "        seq = preds[:L, b].tolist()\n",
    "        flat = []\n",
    "        prev = None\n",
    "        for p in seq:\n",
    "            if p != 0 and p != prev:\n",
    "                flat.append(p)\n",
    "            prev = p\n",
    "        decoded.append(flat)\n",
    "    return decoded\n",
    "\n",
    "def compute_per(ref, hyp):\n",
    "    # Quick Levenshtein if editdistance not installed\n",
    "    try:\n",
    "        import editdistance\n",
    "        d = editdistance.eval(ref, hyp)\n",
    "    except:\n",
    "        return 0 # Skip calculation if lib missing\n",
    "    return d / len(ref) if len(ref) > 0 else 0\n",
    "\n",
    "print(\"Starting Deep Conformer Training...\")\n",
    "best_per = 1.0\n",
    "\n",
    "for epoch in range(1, EPOCHS+1):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    \n",
    "    for batch in train_loader:\n",
    "        feats, in_lens, labels, lbl_lens = batch\n",
    "        feats = feats.to(DEVICE).transpose(0, 1)\n",
    "        labels = labels.to(DEVICE)\n",
    "        lbl_lens = lbl_lens.to(DEVICE)\n",
    "        in_lens = in_lens.to(DEVICE)\n",
    "        \n",
    "        feats = augmenter(feats)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        logits = model(feats, in_lens)\n",
    "        log_probs = F.log_softmax(logits, dim=2)\n",
    "        \n",
    "        loss = criterion(log_probs, labels, in_lens, lbl_lens)\n",
    "        loss.backward()\n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        train_loss += loss.item()\n",
    "        \n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_per = 0\n",
    "    count = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            feats, in_lens, labels, lbl_lens = batch\n",
    "            feats = feats.to(DEVICE).transpose(0, 1)\n",
    "            labels = labels.to(DEVICE)\n",
    "            in_lens = in_lens.to(DEVICE)\n",
    "            lbl_lens = lbl_lens.to(DEVICE)\n",
    "            \n",
    "            logits = model(feats, in_lens)\n",
    "            decoded = greedy_decode(F.log_softmax(logits, dim=2), in_lens)\n",
    "            \n",
    "            idx = 0\n",
    "            for i, hyp in enumerate(decoded):\n",
    "                L_target = lbl_lens[i].item()\n",
    "                ref = labels[idx:idx+L_target].cpu().tolist()\n",
    "                idx += L_target\n",
    "                val_per += compute_per(ref, hyp)\n",
    "                count += 1\n",
    "                \n",
    "    avg_per = val_per / count\n",
    "    print(f\"Epoch {epoch}: Loss={train_loss/len(train_loader):.4f} | Val PER={avg_per:.4f}\")\n",
    "    \n",
    "    if avg_per < best_per:\n",
    "        best_per = avg_per\n",
    "        # Save as a DIFFERENT name\n",
    "        torch.save(model.state_dict(), \"best_conformer_deep.pth\")\n",
    "        print(f\"  >>> New Best Deep Conformer! PER: {best_per:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 13056355,
     "sourceId": 106809,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 31089,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
